---
title: "Recommendation Systems - Capstone Project"
author: "Tim Conze<br><br>"
date: "2024-01-22"
output:
  cleanrmd::html_document_clean:
    theme: water-dark
    toc: true
    highlight: default
    mathjax: default
    fig_caption: true
  pdf_document:
    toc: true
    toc_depth: 3
    number_sections: true
    highlight: haddock
    fig_caption: true
  html_document:
    df_print: paged
    toc: true
    #toc_float: true
    #toc_collapsed: true
    number_sections: true
    theme: sandstone
    highlight: pygments
    code_folding: show
    fig_caption: true
   # tufte::tufte_handout:
  # toc: true
  # number_sections: true
  # highlight: monochrome
toc-title: "Table of Contents"
#knit: pagedown::chrome_print
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


\
\
<br>
<br>



# 1 Introduction
\

Recommender algorithms silently work in the background, giving personalized suggestions specifically catering to a user, increasing the likelihood of their continued usage of a service, thus greatly impacting the average consumer on a daily basis. In the case of personalized ads, they might introduce users to new products they could potentially have interest in, based on their prior behaviors. Hence, such systems can be considered one of the most impactful data science applications, although they are rarely thought of as such, especially in general consensus.

This conundrum is easily explained when exploring the inherent nature of every recommendation system. Their success is measured in their prediction accuracy. This, combined with the consumers natural tendencies to notice inaccuracies more, leads to successful recommendation systems being only subconsciously perceived. In cases in which a correct prediction is given, the enduser will not question the recommendation. On the other hand, inaccurate predictions stand out, leading the user to question the underlying system. This psychological phenomenon is described as “negativity bias”.

Recommendation systems improve constantly, becoming increasingly more accurate and computationally efficient. Their current capabilities, paired with the negativity bias and over exposure of these systems, explain why their impact becomes diluted.

A wide range of applications put recommendation systems into practice. They are implemented by streaming platforms (e.g. Netflix, Disney+), e-commerce (e.g. Amazon, Ebay), social media (TikTok,YouTube), and search engines (e.g. Google) in the form of personalized advertisements, to briefly highlight a few.

\
Going forward, the core concepts needed to understand how recommendation systems are built will be summarized. After outlining the general ideas behind the approaches shown, the first recommendation system model will be built using a baseline approach. Several different methods will be showcased and compared, before ultimately judging them based on their prediction accuracy.\
This analysis will use data collected and provided by GroupLens; a research lab situated at the University of Michigan. They collected millions of movie reviews and offer their data in various different sizes to the public. We will use the 10M (10 million rows) version of the dataset, titled ‘MovieLens’, a rating dataset containing 10.677 user ratings for 69.878 movies released prior to 2009. This dataset gained popularity as a subject for testing and illustrating models. It is very similar to the dataset used in a competition held by Netflix, with a one million dollar prize.\
Loss functions are commonly used to describe a models performance. Since the RMSE was used to compare the models by the participants, the same error metric will be applied, in an effort at beating the target RMSE set by the winning team. This regression approach evaluates the closeness of the predictions to the unknown “true” values. The formula pertaining the Root Mean Squared Error (RMSE) will soon be explored.


The goal of this project is to find and build the _best performing model_ that most accurately predicts unknown ratings of users in the dataset. Ultimately, we want to improve the RMSE achieved by the Netflix Challenge’s winning team with a reproducible and easily implemented algorithm. Moreover, this project will aim at building a _computational efficient algorithm_, since the Netflix challenge has shown computationally expensive algorithms cannot be reasonably implemented in practice. This proves computational efficiency to often be a more important trait than pure accuracy. 


\
\

# 2 Abstract
\

A movie recommender system is an approach to filtering or predicting the users’ movie preferences based on their previous selections and behaviors, using machine learning. It is an advanced filtration mechanism that predicts the possible choices of the user in question and their preferences towards a domain-specific item, in this case, movies.


There are several approaches to giving recommendations. The two most common and popular categories of the machine learning algorithms used for movie recommendations include content-based filtering, and collaborative filtering systems. It is also common to see these two approaches being used in unison.

\

**Content-Based Filtering:**

Content-based filtering approaches utilize a series of discrete, pre-tagged characteristics of an item in order to recommend additional items with similar properties. These characteristics are based on the description of the item, which can be found in meta data or side information. Keywords are then used to describe the items, and a user-profile is built to indicate the type of item this user is inclined towards. \
These recommenders treat recommendations as a user-specific problem, and learn a classifier for the user’s likes and dislikes based on an item’s features.
These algorithms are best suited to situations where there is known data on an item, but not the user.


To abstract the features of the items in the system, an item presentation algorithm is applied. A widely used algorithm is the _tf-idf_ representation, which is also called _vector space representation_. The system creates a content-based profile of users based on a weighted vector of item features. These weights denote the importance of each feature to the user, and can be computed from individually rated content vectors using a variety of techniques. To estimate the probability that the user is going to like the item, simple approaches such as using the average values can be implored, whereas more sophisticated methods make use of machine learning techniques.


\
**Collaborative filtering systems:**

Collaborative filtering approaches build a model from a user’s past behavior, as well as similar decisions made by other users. This model is then used to predict items that the user may have interest in. \
The general assumption we are making is that people who agreed in the past will agree in the future, meaning, they will like similar sorts of items they have previously liked.


\
Collaborative filtering algorithms are divided into two categories:

- _User-based collaborative filtering_ \
Here we look for similar patterns in item preferences in the target user and other users in the database. For this type of recommendation system, users are the focus. For each target item similarities between new and existing users are first calculated, using Euclidian distance, Cosine distance or another metric depending on the algorithm. This is done by filtering the interests of a particular user, by collecting data from many users (collaborating) chosen based similarity above a specified threshold.


- _Item-based collaborative filtering_ \
The basic concept here is to look for similar items that target users rate or interact with. In contrast to the user-based approach, the focus here lies on the items. For each two products, the similarity between them is calculated. Based on a specified threshold the most similar products are then identified.


Each type of system has its strength and weaknesses. The Collaborative filtering approaches require a large amount of information about a user to make accurate recommendations. This is called the “cold start” problem, and is common among collaborative filtering algorithms. On the other hand, content-based filtering requires very little information to start, but is far more limited in its scope, as it can only recommend items that are similar to the original seed, for instance.


\
\
**Matrix Factorization:** \


Collaborative filtering methods are classified as memory-based and model-based. A well-known example of memory-based approaches is the aforementioned user-based algorithm, while an example for a model-based approach is matrix factorization.
\
The idea behind matrix factorization is to represent users and items in a lower dimensional latent space. It is very much related to _factor analysis_, _singular value decomposition (SVD)_, and _principal component analysis (PCA)_.
\
Matrix factorization models work by decomposing the user-item interaction matrix into the product of two lower dimensional rectangular matrices. We can then use the residuals and the structure of these matrices to discern patterns.
\
\
The approach is to approximate the rating matrix $R_{mxn}$  by the product of two matrices containing lower dimensions, $P_{kxn}$ and $Q_{kxn}$  in a way that:
$$R = P'Q$$

If $p_u$ is the $u$-th column of $P$, and  is the $i$-th column of $Q$, then the movie rating placed by the user $u$ on the item $i$ would be predicted as $p_uq_i$ .
The equation is given by the below optimization problem:
$$\displaystyle \min_{P, Q} \sum_{(u,i)\in R} \left[f(p_u,q_i;r_{u,i})+\mu_P||p_u||_1 + \mu_Q||q_i||_1 +  \frac{\lambda_P}{2}||p_u||_2^2 + \frac{\lambda_Q}{2}||q_i||_2^2\right]$$


Where the $u$,$i$ are the locations of the real entries in the $R$, $r_{u,i}$ is the real rating, $ƒ$ is the loss function, and $\mu_P$ , $\mu_Q$ , $\lambda_P$ , $\lambda_Q$  are the usual penalization parameters used by many algorithms to avoid overfitting. 

The procedure of solving the matrix $P$ and $Q$ is the model training, and the process of choosing the penalization parameters is the hyper parameter tuning. After obtaining $P$ and $Q$ we can predict:
$$\hat{R}_{u,i} =p_uq_i$$


\
\
**Error Metrics:**


To enumerate the performance of any given machine learning algorithm, we have a wide array of error metrics at our disposal. It is indispensable to use such a performance metric when building and testing models, since it makes the quantification of the model’s performance reproducible and comparable.

Most commonly, _loss functions_ are used to evaluate a model’s performance. Loss functions, or “Cost functions”, are mathematical functions that calculate the difference, or error, between the model’s predictions and the true values. These functions map an event or values of one or more values onto a real number intuitively representing some “cost” associated with the event. We always strive to minimize a loss function.

\
The most common loss functions in Machine-Learning are:

- _Mean Absolute Error/L1 Loss (MAE)_\
The _mean absolute error_ is measured as the average of the sum of absolute differences between predictions and actual observations. It measures the magnitude of error without considering their direction. _MAE_ uses linear programming to compute the gradients and is robust to outliers, since it does not make use of squares.

- _Mean Square Error/Quadratic Loss/ L2 Loss (MSE)_ \
The _mean square error_ is measured as the average squared difference between predictions and actual observations. Similarly to the _MAE_, it is only concerned with the average magnitude of error irrespective of their direction. However, due to squaring, predictions that are far away from actual values are penalized heavily in comparison to less deviated predictions. Additionally, _MSE_ make its easier to calculate gradients, due to its mathematical properties.

- _Root Mean Square Error (RMSE)_ \
The _root mean square error_, or _root mean square deviation_, is one of the most commonly used measures for evaluating the quality of predictions. It shows how far predictions fall from the measured true values using Euclidean distance.
_RMSE_ is commonly used in supervised learning applications, as _RMSE_ uses and needs true measurements at each predicted data point.


There are many more loss functions that are of value in different use cases and types of problems, as they each offer their own strength and weaknesses.

We will employ the _root mean square error_ as our evaluation metric throughout this analysis. Since this loss function was also used to decide on the winner of the Netflix challenge, it will ease comparison to the target goal.
To compute the _RMSE_, the residuals (difference between prediction and true values) are calculated for each data point, the norm of residual is computed for each data point, the mean of residuals is computed, after which the square root of that mean is taken. 

That equates to this simple formula:
$$ RMSE = \sqrt{ \frac{1}{n} \sum_{i = 1}^n (X_i - x_0)^2 }$$


\
\
\

# 3 Methodology
\

In this section we will attempt to explain the core concepts, and ideas, behind the approaches that we will later use to build recommendation models. This is by no means an exhaustive description of the underlying concepts and only serves as a simplified summary. Very little detail of the mathematics will be given, and only the most vital formulas will be highlighted.


\
\
**Baseline Models:**

The methodology for the baseline models are unsurprisingly simple. We will predict the same rating for all movies, regardless of the user. \
A model that assumes the same rating for all movies and users with all differences explained by random variation would look like the following:

$$Y_{u,i} = \mu + \epsilon_{u,i}$$

With $\epsilon$ independent errors sampled from the same distribution, centered at 0 and $\mu$ the “true” rating for all movies. We use the notation “$u$” and “$i$” for users and movies respectively, and will do so going forward.\
We know that the estimate that minimizes the RMSE is the least square estimate of $\mu$ would be the average of all ratings in this case. That is the first baseline model we will employ.

The second baseline model we employ will be guessing just 3. We know that ratings are generally more positive than negative, so instead of guessing 2.5 (median of 5), we will slightly overestimate. We expect the result of this model to be worse than that of the first baseline model’s.

To complete the triad of baseline models, a model that relies on randomized guesses will be added. It will assist in judging how bad a model can be without any information, except for the range of possible outcomes, and how much better the other baseline models can fare in comparison.


\
\
**Modeling Movie Effects:**

As previously stated, we know from experience that some movies are generally rated higher than others. This intuition, that different movies are rated differently, is confirmed by data. We can augment our previous model by adding the term $b_i$ to represent the average rating for movie $i$:

$$Y_{u,i} = \mu + b_i + \epsilon_{u,i}$$


We know that the least squares estimate of $\hat{b_i}$ is the average of $Y_{u,i}$ - $\hat{\mu}$ for each movie $i$. So we can compute them by subtracting the $\mu$ from each rating, resulting in the residuals $b_i$. If the $\mu$ is about 3.5, a $b_i$ value of 1.5 would imply a perfect rating.


\
\
**Modeling User Effects:**

We can apply the same approach to the user effect. It can be assumed that each user impacts the ratings differently, similarly to the movies. This implies that we can further improve our model by adding the user effect as follows:
$$Y_{u,i} = \mu + b_i + b_u + \epsilon_{u,i}$$

Here, the $b_u$ is a user-specific effect. If a user that usually rates negatively (negative $b_u$) rates a great movie(positive $b_i$) these effects counter one another and we may be able to correctly predict that this user gave this great movie a 3 rather than a 5.\
Similarly to before, we can estimate $\hat{b_u}$ as the average of $y_{u,i} - \hat{\mu} - \hat{b_i}$ .


\
\
**Modeling all other Effects:**

We can extrapolate this concept to include additional effects found in the data. During the analysis we will progressively include the genre effect and multiple time effects.

$$Y_{u,i} = \mu + b_i + b_u + b_g + b^t + \epsilon_{u,i}$$

$b_g$ denoting the genre effect, while $b^t$ denotes the time effects. Later, the time effects will be split into separate year- , month- , week- , day- , hour- , minute – effects denoted by $b_y$,$b_m$,$b_w$,$b_d$,$b_h$,$b_{mi}$.


\
\
**Regularization:**

The dataset includes many movies that were rated by very few users, and small sample sizes lead to uncertainty. Therefore, larger estimates of $b_i$, negative or positive, are more likely. These are noisy estimates that should not be trusted, especially pertaining predictions. Large errors can increase the RMSE, meaning it is wise to be conservative when unsure.

For that reason, we introduce the concept of regularization. Regularization permits us to penalize large estimates that are formed using little information. 

The general idea behind regularization is to constrain the total variability of the effect sizes. To understand why that is useful, consider a case in which we have one movie ($i$ = 1) with 100 user ratings and 4 movies ( $i$ = 2,3,4,5) with a single user rating. Suppose we know the average rating is $\mu$ = 3. If we use least squares, the estimate for the first movie effect $b_i$ is the average of the 100 user ratings, $\displaystyle 1/100 \sum_{i = 1}^{100} \left(Y_{u,i} - \mu\right)$, which we expect to be quite precise. However, the estimate for the other movies will simply be the observed deviation from the average rating, which is an estimate based on a single number, therefore, it will not be precise whatsoever.

Note that these estimates make the error $Y_{u,i} - \mu + \hat{b_i}$ equal to 0 for $i$ = 2,3,4,5 , but this is a case of over-training. In fact, ignoring the one user and guessing that movies 2,3,4,5 are just average movies ($b_i = 0$) might provide a better prediction. The general idea of penalized regression is to control the total variability of the movie effects: $\displaystyle \sum_{i = 1}^{5} b_i^2$. Specifically, rather than minimizing the least squares equation, we minimize an equation that adds a penalty:

$$\displaystyle \sum_{u,i}\left(y_{u,i} - \mu - b_i\right)^2 + \lambda \sum_{i} b_i^2$$

The first term is the sum of squares, the second is a penalty that gets larger when many $b_i$ are large. Using calculus, we can show that the values of $b-i$ that minimize this equation are:

$$\displaystyle \hat{b_i}(\lambda) = \frac{1}{\lambda + n_i} \sum_{u = 1}^{n_i}\left(Y_{u,i} - \hat{\mu}\right)$$

,where $n_i$ is the number of ratings made for movie $i$. This approach will have the desired effect: when the sample size $n_i$ is very large, a case that gives us a stable estimate, then the penalty $\lambda$ is effectively ignored since $n_i + \lambda \approx n$ . However, when the $n_i$ is small, then the estimate $\hat{b_i}(\lambda)$ is shrunken towards 0. The larger the $\lambda$, the more we shrink.\
To select the ideal value of $\lambda$, cross-validation will be utilized.


\
\
**Matrix Factorization:**

Matrix Factorization is an advanced approach to recommendation systems and is related to _factor analysis_, _singular value decomposition (SVD)_, and _principal component analysis (PCA)_.\
We have described the model:
$$Y_{u,i} = \mu + b_i + b_u + \epsilon_{u,i}$$

This model accounts for movie to movie differences (movie effects) through the $b_i$ and user to user differences (user effects) through the $b_u$, but this model leaves out an important source of variation related to the fact that groups of movies, as well as groups of users, have similar rating patterns. 

To discover these patterns, the residuals must be studied:
$$r_{u,i} = y_{u,i} - \hat{b_i} - \hat{b_u}$$

If the movie and user effects explain all the signal, and the $\epsilon$ (Error) is merely noise, then the residuals for different movies should be independent of one another. But, that is not the case..\
There is structure in the data. Some users generally prefer action movies, others may prefer romantic movies. Beyond that, some users may prefer movies with one particular actor over another, or, tend to dislike a movie once it has garnered wide appeal.\


To discern the patterns and the structure, we use factor analysis. We compute the highly correlated residuals, from which we create two vectors, $p$ and $q$, to explain the observable structure. These vectors, split the movies($q$) and the users($p$) into two separate groups. For example, splitting them into action movies/non-action movies, and action movie fans/non-action movie fans. The main point is that we can almost reconstruct $r$ with a couple of vectors, containing far less values.\
If $r$ contains the residuals for our users and movies, we can write the following mathematical formula for our residuals $r_{u,i} \approx p_uq_i$.

This implies more variability can be explained by modifying our previous model to:
$$Y_{u,i} = \mu + b_i + b_u + p_uq_i + \epsilon_{u,i}$$

The structure we find in data is usually very complex and we need many factors to accurately represent the underlying structure. To explain the complex correlation we observe in real data, we permit the entries of $p$ and $q$ to be continuous values.\
In practice we use as many factors as needed to explain most of the variability in the data.

$$Y_{u,i} = \mu + b_i + b_u + p_{u,1}q_{1,i}+ p_{u,2}q_{2,i} + ... +  p_{u,n}q_{n,i} + \epsilon_{u,i}$$

\
The decomposition of the residuals into the multiple $p$ and $q$ vectors is strongly related to _SVD_ (Singular value decomposition) and _PCA_ (Principal component analysis). _SVD_ and _PCA_ are complicated concepts; simply put, _SVD_ is an algorithm that finds the vectors $p$ and $q$ that permit us to rewrite the matrix $r$ with $m$ rows and $n$ columns as:

$$r_{u,i} = p_{u,1}q_{1,i}+ p_{u,2}q_{2,i} + ... +  p_{u,n}q_{n,i}$$

,with the variability of each term decreasing and with the $p$’s uncorrelated. We call the $q$ vectors the _principal components_, while the $p$ vectors are the _user effects_. The algorithm also computes this variability so that we are able to deduce how much of the matrices total variability is explained as we add new terms. Generally, we can explain most of the variability with just a few terms.


\
\
\

# 4 Preparing our Analysis

\


## 4.1 Data Loading and Preparation

\
Before the analysis, we must **download the MovieLens dataset** we will be working with. We convert the data into a dataframe, after which we create a working (edx) and final validation set. Those will be used to build our models and to test our final algorithm on, respectively. We also **download the necessary packages** needed to conduct the wrangling of our dataset and **load the libraries** we will be using.


```{r downloading data and creating train and test set, eval = T, results = "hide", class.source = "fold-hide", message =F, warning= F, error =F}

# Note: this process could take a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

options(timeout = 120)

dl <- "ml-10M100K.zip"
if(!file.exists(dl))
  download.file("https://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings_file <- "ml-10M100K/ratings.dat"
if(!file.exists(ratings_file))
  unzip(dl, ratings_file)

movies_file <- "ml-10M100K/movies.dat"
if(!file.exists(movies_file))
  unzip(dl, movies_file)

ratings <- as.data.frame(str_split(read_lines(ratings_file), fixed("::"), simplify = TRUE),
                         stringsAsFactors = FALSE)
colnames(ratings) <- c("userId", "movieId", "rating", "timestamp")
ratings <- ratings %>%
  mutate(userId = as.integer(userId),
         movieId = as.integer(movieId),
         rating = as.numeric(rating),
         timestamp = as.integer(timestamp))

movies <- as.data.frame(str_split(read_lines(movies_file), fixed("::"), simplify = TRUE),
                        stringsAsFactors = FALSE)
colnames(movies) <- c("movieId", "title", "genres")
movies <- movies %>%
  mutate(movieId = as.integer(movieId))

movielens <- left_join(ratings, movies, by = "movieId")

# Final hold-out test set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding") # if using R 3.6 or later
# set.seed(1) # if using R 3.5 or earlier
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in final hold-out test set are also in edx set
final_holdout_test <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from final hold-out test set back into edx set
removed <- anti_join(temp, final_holdout_test)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)
```


\
\

## 4.2 Downloading Packages and Loading Libraries

\
We will also create further train and test sets for later use and download all other packages that we will make use of during our analysis. Afterwards, we will load the libraries into our working environment.
One of the packages we will download and load is the nonobligatory `doParallel`
package. It will help us to run the code we write more efficiently, enabling us to use multiple cores at once.


```{r downloading packages and loading libraries, eval = T, class.source = "fold-hide", message =F, warning= F, error =F, results = "hide"}

### Libraries to download:
if(!require(tidyverse)) install.packages("tidyverse",repos ="http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret",repos ="http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate",repos ="http://cran.us.r-project.org")
if(!require(rmarkdown)) install.packages("rmarkdown",repos ="http://cran.us.r-project.org")
if(!require(dplyr)) install.packages("dplyr",repos ="http://cran.us.r-project.org")
if(!require(tidyr)) install.packages("tidyr",repos ="http://cran.us.r-project.org")
if(!require(ggplot2)) install.packages("ggplot2",repos ="http://cran.us.r-project.org")
if(!require(stringr)) install.packages("stringr",repos ="http://cran.us.r-project.org")
if(!require(scales)) install.packages("scales",repos ="http://cran.us.r-project.org")

if(!require(widyr)) install.package("widyr",repos ="http://cran.us.r-project.org")
if(!require(knitr)) install.package("knitr",repos ="http://cran.us.r-project.org")
if(!require(gghighlight)) install.package("gghighlight",repos ="http://cran.us.r-project.org")
if(!require(RColorBrewer)) install.packages("RColorBrewer",repos ="http://cran.us.r-project.org")
if(!require(kableExtra)) install.packages("kableExtra",repos ="http://cran.us.r-project.org")

# Used in data exploration:
if(!require(wordcloud)) install.package("wordcloud",repos ="http://cran.us.r-project.org") 

# Uncomment to download parallel computing package
# install.package("doParallel") 


# Later used in Matrix Factorization:
if(!require(recosystem)) install.package("recosystem",repos ="http://cran.us.r-project.org")


# Later use in recommenderlab models
if(!require(recommenderlab)) install.packages("recommenderlab",repos ="http://cran.us.r-project.org")
if(!require(reshape2)) install.packages("reshape2",repos ="http://cran.us.r-project.org")


### Loading Libraries:

library(tidyverse)
library(caret)
# library(doParallel) # If wanted
library(tidyr)
library(scales)
library(widyr)
library(knitr)
library(wordcloud)
library(recosystem)
library(kableExtra)
```

\
\

## 4.3 Creating Further Test and Train Sets

\
The last step in our data preparation will be splitting our previously created train set into further test and train sets. We will make use of them during our model building process. We make sure to **not** use the final validation in any of the model testing, so as to not introduce any bias that could affect the results.

```{r extra train and test sets, eval = T, class.source = "fold-hide", message =F, warning= F, error =F, results = "hide"}


# Putting years in separate column

edx_dat <- edx %>% mutate(year = str_extract(title, "(\\(\\d{4}\\))"),
                          title = str_replace(title,"(\\(\\d{4}\\))$","")) %>%
  mutate(year = as.numeric(str_extract(year,"\\d{4}")))


## Test and train (Used for overall testing and to find final models lambda)

test_index <- createDataPartition(y = edx_dat$rating, times = 1, p = 0.1, list = FALSE)

edx_train <- edx_dat[-test_index,]
temp <- edx_dat[test_index,]

edx_test <- temp %>% 
  semi_join(edx_train, by = "movieId") %>%
  semi_join(edx_train, by = "userId")

removed <- anti_join(temp, edx_test)
edx_train <- rbind(edx_train, removed)

rm(test_index, temp, removed)


## Test and train set to find proper testing lambda


test_index <- createDataPartition(y = edx_train$rating, times = 1, p = 0.1, list = FALSE)

edx_train_l <- edx_train[-test_index,]
temp <- edx_train[test_index,]

edx_test_l <- temp %>% 
  semi_join(edx_train_l, by = "movieId") %>%
  semi_join(edx_train_l, by = "userId")

removed <- anti_join(temp, edx_test_l)
edx_train_l <- rbind(edx_train_l, removed)

rm(test_index, temp, removed)

```

\
Reasoning for creating two splits: \
The difference between using the actual sets, using only one test/train set and the approach used here with two test/train sets to find the lambda during regularization is negligible and they all yield the same result. The decision to split the dataset twice was made in order to accurately simulate a situation where unknown ratings need to be predicted. The first set is our primary dataset for testing our models, while the second set was created from the first to calculate the lambdas without introducing bias. \
This explanation will be reiterated later, when they are in use.


\
\
\

# 5 Exploratory Data Analysis

\
One must familiarize themselves with the data at hand before every data science project. It is essential to know where the important information lies, in what form they are being stored as within the dataset, and what possible wrangling problems could arise throughout the analysis. \
We also aim to find the significant features for the analysis, and how to properly make use of them during model creation.

\
\
To begin the data exploration we will examine the data, and how it is being stored:

```{r quick look at dataset, eval = T}
head(edx)
```

\
At first glance, we see that our data is being stored in a dataframe with 6 columns. The first three of those being most essential to our analysis. They store the users’ identification numbers (userId), the movies’ identification numbers (movieId), and their affiliated ratings (rating), respectively. We also find three additional features at our disposal, the exact time each rating was submitted in the form of a `datetime` object (timestamp), the full title of the movie rated including its individual release year (title), and a group of genres corresponding to each movie where each genre is separated by a pipe symbol (genres).

\
Our dataset stores 9.000.055 ratings (rows) and the aforementioned 6 features (columns). We have a huge set of datapoints with many users rating many movies.
The ratings are created by a combination of 10.677 unique movie Id’s and 69.878 unique user Id’s.
We should be able to build a fairly accurate movie recommendation system judging by the vast number of datapoints, and the fairly limited amount of movies compared to users.

```{r doParallel, eval = T, include = F}
library(doParallel)
# detectCores() # - to find out number of cores available to you
registerDoParallel(cores = 8) # Device with 10 cores 
                              # (only incremental change the more cores are used)

```

```{r rows, columns,unique movie Ids, unique userIds, eval = T, class.source = "fold-hide", results ="hide"}
# How many columns and rows?
nrow(edx)
ncol(edx)

## How many unique movies?
n_distinct(edx$movieId)

## How many unique users?
n_distinct(edx$userId)
```

\

We can also see that we are working with a very sparse dataset. This could lead to problems depending on what methods we employ. \
To visualize the sparsity, we can see a sample of movies here:

\
```{r sparsity, eval = T, echo = F, message =F, warning= F, error =F, dev.args =list(bg = "azure")}
users <- sample(unique(edx$userId), 100)
edx %>% filter(userId %in% users) %>% 
  dplyr::select(userId, movieId, rating) %>%
  mutate(rating = 1) %>%
  pivot_wider(names_from = movieId, values_from = rating) %>% 
  (\(mat) mat[, sample(ncol(mat), 100)])()%>%
  as.matrix() %>% 
  t() %>%
  image(1:100, 1:100,. , xlab="Movie ID", ylab="User ID")
abline(h=0:100+0.5, v=0:100+0.5, col = "cornsilk")

```

\
\

## 5.1 Most Rated Movies

\
Lets take a look at the most rated movies in our dataset:

\
```{r most rated movies plot, eval = T, echo = F, message =F, warning= F, error =F, dev.args =list(bg = "azure")}
edx %>% 
  group_by(title) %>% 
  summarize(count = n()) %>% 
  arrange(desc(count)) %>% 
  top_n(20,count) %>% 
  ggplot(aes(count,reorder(title,count),fill = count)) +
  geom_bar(stat = "identity")+
  scale_fill_gradient2(low="darkseagreen",high = "azure3", mid = "antiquewhite4", midpoint = 25000)+
  xlab("Count")+
  ylab(NULL) +
  theme_minimal()+
  ggtitle("Most Rated Movies")+
  theme( plot.title = element_text(size = (10)),
           panel.background = element_rect(fill = "cornsilk"))+
  scale_x_continuous(labels = unit_format(unit = "K", scale = 1e-3))

```

\
To no surprise, these top rated movies are older classics and blockbuster movies. They are all critically acclaimed movies that enjoy wide appeal. Apart from that, enough time has passed since their release date to be viewed, and rated by many users.

\
\

## 5.2 Ratings

\
Next, we will look at the ratings in more detail. Possible ratings range from 0.5 to 5, with 5 being a perfect rating, and 0.5 being the lowest a user can rate a movie. Ratings increase in half values starting at 0.5, so the total number of rating choices is 10. There are not any ratings of 0 in the dataset.

The ratings are distributed as follows:

```{r most common ratings table, eval = T, echo = F, message =F, warning= F, error =F}
edx %>% group_by(rating) %>% mutate(count = n()) %>% arrange(desc(count)) %>% 
  distinct(rating,count) %>% kable()

```

\
As we can see, half value ratings are generally less common compared to their full value rating counterparts. Furthermore, we see a bias towards positive ratings, being that negative ratings are more rare. Ratings of 4 are the most common, followed by ratings of 3.

\
```{r ratings distribution plot, eval = T, echo = F, message =F, warning= F, error =F,dev.args =list(bg = "azure")}
edx %>%
  group_by(rating) %>%
  summarize(count = n()) %>%
  ggplot(aes(x = rating, y = count)) +
  geom_line(col = "coral")+
  xlab("Rating")+
  ylab("Count")+
  theme(panel.background = element_rect(fill = "cornsilk"),plot.background = element_rect(fill = "azure", colour ="azure"))+
  scale_y_continuous(labels = unit_format(unit = "M", scale = 1e-6))
```

\
We see clear tendencies in the data. Users tend to rate positively. The most common ratings are positive, though they are not perfect ratings of 5, exemplifying the users' hesitancy to give a perfect score. Notably, half ratings are also less common.
\
We get a very clear picture when we take each user's rounded, average rating and explore how often each rating occurs:

\
```{r rounded ratings distribution plot, eval = T, echo = F, message =F, warning= F, dev.args =list(bg = "azure")}
edx %>% group_by(userId) %>% 
  mutate(rating = mean(rating)) %>% 
  ungroup() %>% 
  select(userId,rating) %>% 
  distinct() %>% 
  group_by(rating) %>%
  mutate_at("rating",round,1) %>% 
  summarize(n = n()) %>% 
  ggplot(aes(rating,n))+
  xlab("Rating")+
  ylab("Times rated")+
  geom_point(color = "coral")+
  theme( plot.background = element_rect(fill = "azure", colour ="azure"))

```

\
\

**User Activity:**
\
Magnifying the distribution of user activity, we can observe that the vast majority of users rated less than 1000 movies. Naturally, the number of users dissipates the more we ascend in rating volume, with only a few outliers that rated more than 4000 movies.

\
```{r user activity plot, eval = T, echo=F, dev.args =list(bg = "azure")}
edx %>% 
  group_by(userId) %>%
  summarize(n = n()) %>% 
  ggplot(aes(userId,n))+
  geom_point(alpha = 0.1)+
  theme(axis.text.x = element_blank())+
  xlab("Users")+
  ylab("Rated Movies")+
  theme(panel.background = element_rect(fill = "cornsilk"), plot.background = element_rect(fill = "azure",colour = "azure"))+
  ggtitle("User Activity")
```

\
\
**Times rated by average rating:**
\
When we stratify by each user’s average rating, taking the amount of times each user has rated into account, and observe the distribution of ratings, we see that the majority of users that rated many movies, have an average rating between 3 and 4.

\
```{r number of ratings vs rating mean by userid plot, eval = T,echo=F, dev.args =list(bg = "azure")}
edx %>% 
  group_by(userId) %>% 
  summarize(n = n(), rating = mean(rating)) %>% 
  ggplot(aes(rating,n))+
  geom_point(alpha= 0.3, col = "cadetblue")+
  ggtitle("Rating Sum vs Rating Mean by UserID")+
  xlab("Average Rating")+
  ylab("Times rated")+
  theme( plot.title = element_text(size = (10)),panel.background = element_rect(fill = "cornsilk"), plot.background = element_rect(fill = "azure",colour ="azure"))

```

\
When we in turn look at the times a movie got rated against their respective average rating, we see that movies that were more frequently rated, tend to be rated more positively on average.

\
```{r rating sum vs rating mean by movieId plot, eval = T,echo =F, dev.args =list(bg = "azure")}

edx %>% 
  group_by(movieId) %>% 
  summarize(n = n(), rating = mean(rating)) %>% 
  ggplot(aes(rating,n))+ 
  geom_point(alpha= 0.3, col = "cadetblue")+
  ggtitle("Rating Sum vs Rating Mean by MovieID")+
  xlab("Average Rating")+
  ylab("Times rated")+
  theme( panel.background = element_rect(fill = "cornsilk"), plot.background = element_rect(fill = "azure",colour ="azure"))
  
```


\
\

## 5.3 NA's / Missing Values
\
There are no NA’s in the dataset, discounting the fact that not all users rated every movie. These missing values are not denoted as NA’s and will not cause problems during the analysis, aside from creating sparsity.  \
The only notable NA present in the dataset is the genre “(no genres listed)” in the genre column. It is assigned to a single movie with 7 ratings. Research on the movie “Pull my Daisy” (movieId: 8606) showed that the genre “Drama” would be best suited for it. We decided against changing its genre, since it will not impact the model results.

```{r nas + no genres listed, eval = T,echo=T ,message =F, warning= F, error =F}
### Are there any NA's in the dataset?
sum(is.na(edx$userId))
sum(is.na(edx$movieId))
sum(is.na(edx$timestamp))
sum(is.na(edx$title))

## Ratings that are 0:
sum(edx$rating == 0)

## No genres provided:

sum(edx$genres == "(no genres listed)")
```

\
\

## 5.4 Genres
\
We will now explore the genre feature in detail.\

As noted earlier, we find that movies that have multiple genres assigned to them, genres are split with a “|” delimiter. We will have to split the genres before analyzing them further as shown:

```{r genre splitting example, eval = T, echo = T, message =F, warning= F, error =F}
library(tidyr)

edx %>% 
  mutate(genres =str_split(genres, "\\|")) %>% 
  unnest(cols = c(genres)) %>% 
  group_by(genres) %>% 
  filter(genres %in% c("Comedy","Drama","Thriller","Romance")) %>% 
  summarize(n = n())
```

\
We begin by examining all genres present in the dataset:

```{r all genres, eval = T, echo = F, message =F, warning= F, error =F}
edx %>% 
  mutate(genres =str_split(genres, "\\|")) %>% 
  unnest(cols = c(genres)) %>% 
  group_by(genres) %>% 
  summarize(n = n()) %>% 
  arrange(desc(n)) %>%
  kable()
```

\
\
To see which genres are most commonly assigned to the movies found in the dataset, we count the number of unique movies that include each genre, regardless of how often they were rated:

\
```{r visualization of unique movies per genre, eval = T, echo = F, message =F, warning= F, error =F, dev.args =list(bg = "azure")}

edx %>% 
  select(movieId,genres) %>% 
  unique() %>% 
  mutate(genres =str_split(genres, "\\|")) %>% 
  unnest(cols = c(genres)) %>% 
  group_by(genres) %>% 
  summarize(n = n()) %>% 
  arrange(desc(n)) %>% 
  ggplot(aes(n,reorder(genres,n),fill = n))+
  geom_bar(stat = "identity")+
  scale_fill_gradient2(low="darkseagreen",high = "azure3", mid = "antiquewhite4", 
                       midpoint = 1500,
                       labels = unit_format(unit = "K", scale = 1e-3))+
  xlab("Count")+
  ylab(NULL) +
  theme_minimal()+
  ggtitle("Number of unique Movies by Genre")+
  theme( plot.title = element_text(size = (10)),
         panel.background = element_rect(fill = "cornsilk"))+
  scale_x_continuous(labels = unit_format(unit = "K", scale = 1e-3))+
  labs(fill = "Number of Movies")
```

\
With the inclusion of the previously discussed “(no genre listed)”, we find that there are 20 different genres present. The amount of times a genre occurs varies greatly, as there is a huge discrepancy between the first two most assigned genres and the others. There is also a huge gap between the following few genres and the genres occurring the least.\

The “(no genres listed)” category is the only one that is assigned to only a single movie. We assume that the “Drama” genre will be grouped up most of the time and serve as an additional genre rather than the primary classification, explaining its wide use. The same can be said for similar high occurring genres like “Comedy”.
\
\


We can make use of a wordcloud to better visualize the distribution of the most assigned genres:

```{r wordcloud, eval = T, echo = F, message =F, warning= F, error =F, dev.args =list(bg = "transparent")}
library(wordcloud)

wrd_cloud <- edx %>% mutate(genres =str_split(genres, "\\|")) %>% 
  unnest(cols = c(genres)) %>% 
  group_by(genres) %>% 
  summarize(n = n()) 
wordcloud(words = wrd_cloud$genres, freq = wrd_cloud$n, min.freq = 10, max.words = 10,
            random.order = F, rot.per = 0.35, scale = c(5,0.2), font = 4,
            random.color = F, colors = brewer.pal(8,"Spectral"),
            main = "Most Rated Genres")
```

\
\
\

**Genre Groups:**
\
\
Thereafter, we will explore the genre groups in more detail. First, we look at the percentage of datapoints with multiple genres:

```{r genre group percentage, eval = T, echo= T,class.source = "fold-hide" }
## Percentage of multiple genres supplied for one movie

mean(str_detect(edx$genres, "\\|")) # Percentage of multiple genres

mean(str_detect(edx$genres, "^[\\-A-Za-z]+$")) # Only one genre

# mean(str_detect(edx$genres, "^\\w+\\-*\\w+$")) # Another way of doing the above


```

\
About 19.33% are lone genres while the vast majority of datapoints we find in our dataset have a genre group assigned to them (80.66%).
\
To gain more insight into the genre groups, we will look at their distribution. First at their respective occurrence in the dataset, taking times rated into account, and then we look at the distribution of genre groups that were assigned to each unique movie.

\
```{r visualization of genre group sizes, eval = T, echo = F, message =F, warning= F, error =F, dev.args =list(bg = "azure")}
## Clean genre group sizes:
grp_sz <- edx %>% 
  mutate(genres =str_split(genres, "\\|")) %>% 
  unnest(cols = c(genres)) %>% 
  select(movieId,genres,userId) %>% 
  group_by(movieId,userId) %>% 
  mutate(n_genres = n_distinct(genres)) %>% 
  select(movieId,userId,n_genres) %>% 
  distinct() %>% 
  ungroup() %>% 
  select(n_genres) %>% 
  group_by(n_genres) %>% 
  summarize(n = n()) %>% 
  distinct() %>% 
  arrange(desc(n))
kable(grp_sz)


# Visualization of group sizes:
library(scales)

grp_sz %>% ggplot(aes(factor(n_genres),n)) +
  geom_col(fill = "bisque4")+
  ylab("Count in Millions") +
  xlab("Group Size") +
  theme(axis.text.x = element_text(angle = 90))+
  scale_y_continuous(labels = unit_format(unit = "M", scale = 1e-6))+
  theme_light()+
  theme(panel.background = element_rect(fill = "cornsilk"),
        text = element_text(family = "sans"), plot.background = element_rect(fill = "azure", colour = "azure"))

```


```{r visualization of genre grp_sz by unique movieId, eval = T, echo = F, dev.args =list(bg = "azure")}

edx %>% 
  mutate(genres =str_split(genres, "\\|")) %>% 
  unnest(cols = c(genres)) %>% 
  select(movieId,genres) %>% 
  group_by(movieId) %>% 
  mutate(genres = n_distinct(genres)) %>% 
  distinct() %>% 
  ungroup() %>% 
  select(genres) %>% 
  group_by(genres) %>% 
  summarize(n = n()) %>% 
  distinct() %>% 
  arrange(desc(n)) %>% 
  ggplot(aes(factor(genres),n)) +
  geom_col(fill = "azure4")+
  ylab("Unique Movies") +
  xlab("Group Size") +
  theme(axis.text.x = element_text(angle = 90))+
  theme_light()+
  theme(panel.background = element_rect(fill = "cornsilk"), plot.background = element_rect(fill = "azure", colour = "azure"))+
  ggtitle("Group Size Appearance by Movie ID")

```

\
Groups of 3 are most common in the dataset when accounting for how often they were rated. When looking exclusively at unique movies, and comparing the genre groups of movies we find in our dataset irrespective of how often they were rated, we find that the genre group size including the most movies is 1. There is only a single movie with a genre group size of 8, and big groups are generally very rare.

\
\
\

**Lone Genre Appearance:**
\
\
It might be of interest to see which genres occur alone most frequently. For that, we will look at a plot that visualizes the lone genre appearance count:

\
```{r visualization of lone genres, eval = T, echo = F, dev.args =list(bg = "azure")}
edx %>% 
  mutate(genres =str_split(genres, "\\|")) %>% 
  unnest(cols = c(genres)) %>% 
  select(movieId,genres) %>% 
  group_by(movieId) %>% 
  mutate(n_genres = n_distinct(genres)) %>% 
  distinct() %>% 
  ungroup() %>% 
  select(n_genres,movieId,genres) %>% 
  group_by(n_genres) %>% 
  filter(n_genres == 1) %>% 
  ungroup() %>% 
  distinct() %>% 
  select(genres) %>% 
  group_by(genres) %>% 
  summarize(n = n()) %>% 
  arrange(desc(n)) %>%
  ggplot(aes(n,reorder(genres,n)))+
  geom_col() +
  theme(panel.background = element_rect(fill = "cornsilk"), plot.background = element_rect(fill = "azure", colour ="azure"))+
  xlab("Count")+
  ylab("Genre")+
  ggtitle("Lone Appearance Count")
```

\
Our previous assumption was proven wrong, as the genres “Drama” and “Comedy” rank first and second in lone appearances. “Documentary” and to a lesser extent, “Horror” are notable for ranking third and fourth in lone genre appearance, while being at the bottom of the list in general appearance.

\
\
\

**Most common Genre Pairs:**
\
\
Here we see the most common genres that appear together. Unsurprisingly, the two most common genres are the most commonly occurring pair. We see no surprises at the top of this list as all pairs seem to make sense together.

\
```{r most common genre pairs, eval = T, echo = F,,message =F, warning= F, error =F, dev.args =list(bg = "azure")}
edx %>% 
  mutate(genres =str_split(genres, "\\|")) %>% 
  unnest(cols = c(genres)) %>% 
  select(movieId,genres) %>% 
  pairwise_count(.,genres,movieId) %>% 
  arrange(desc(n)) %>% 
  top_n(10) %>% 
  unite(col = genres,"item1","item2", sep = "+") %>% 
  ggplot(aes(n,reorder(genres,n)))+
  geom_col() +
  theme(panel.background = element_rect(fill = "cornsilk"), plot.background = element_rect(fill = "azure", colour = "azure"))+
  ggtitle("Most Common Genre Pairs")+
  xlab("Appearances Together")+
  ylab("Genre Pairs")

```

\
We will also take a quick look at the correlation between genres:

\
```{r cor between genres, eval = T, echo = F}
edx %>% 
  mutate(genres =str_split(genres, "\\|")) %>% 
  unnest(cols = c(genres)) %>% 
  select(movieId,genres) %>% 
  pairwise_cor(.,genres,movieId) %>% 
  arrange(desc(abs(correlation))) %>% 
  head() %>% 
  kable()

```

\
No correlation between genres could be found. The only somewhat notable correlation was the one between “Animation” and “Children”.

\
\
\

**Genre Performance:**
\
\
We will now take a closer look at each genres average rating, or its “performance”. We assume that some genres are generally better received than others. We also add the rank of occurrence into our table to see how their average rating compares to how often genres appear.

```{r genre performance table, eval = T, echo = F, message =F, warning= F, error =F}
g_p <- edx %>% 
  mutate(genres =str_split(genres, "\\|")) %>% 
  unnest(cols = c(genres)) %>% 
  group_by(genres) %>% 
  summarize(n = n(), rating = mean(rating)) %>% 
  arrange(desc(rating)) %>% 
  mutate(ranking = rank(-n))
kable(g_p)
```

\
Initially one would think genres that appear less, rank higher in rating. We see two genres at the top that rank fairly low in appearance count. In contrast, we also see “Drama” having a fairly high average rating, while it is the most commonly occurring genre in the dataset. The correlation between rating and size also seems to suggest that both variables are independent of each other.

Furthermore, most genres have very similar average ratings, which is to be expected.

Some notable exceptions are:

- _Horror_ : Can have bad production quality; scary movies can be especially badly received if viewer is not a fan of the genre
- _Film-Noir_ : Niche; viewer might choose with more intent/ be less critical
- _DOC/War_ : Niche; only has to accurately portray events
- _IMAX_ : Usually better production and blockbuster movies

\

```{r genre avg rating plot, eval = T, echo = F, dev.args =list(bg = "azure")}
edx %>% 
  mutate(genres =str_split(genres, "\\|")) %>% 
  unnest(cols = c(genres)) %>% 
  group_by(genres) %>% 
  summarize(rating = mean(rating)) %>% 
  ggplot(aes(reorder(genres,rating),rating,fill = genres))+
  geom_col()+
  coord_cartesian(ylim = c(3,5))+
  theme(axis.text.x = element_text(angle=70,hjust = 1,size = 5))+
  theme(panel.background = element_rect(fill = "cornsilk"), plot.background = element_rect(fill = "azure", colour ="azure")) +
  theme(axis.title.x = element_blank())+
  ylab("Rating")+
  ggtitle("Genres' Average Rating")
```


```{r ratings per genre plot, eval = T, echo = F, dev.args =list(bg = "azure")}
edx %>% 
  mutate(genres =str_split(genres, "\\|")) %>% 
  unnest(cols = c(genres)) %>% 
  group_by(genres) %>% 
  summarize(n = n()) %>% 
  arrange(desc(n)) %>% 
  ggplot(aes(n,reorder(genres,n),fill = n))+
  geom_bar(stat = "identity")+
  scale_fill_gradient2(low="darkseagreen",high = "azure3", mid = "antiquewhite4", 
                       midpoint = 1500000,
                       labels = unit_format(unit = "M", scale = 1e-6))+
  xlab("Count")+
  ylab(NULL) +
  theme_minimal()+
  ggtitle("Most Rated Genres")+
  theme( plot.title = element_text(size = (10)),
         panel.background = element_rect(fill = "cornsilk"), plot.background = element_rect(fill = "azure", colour = "azure"))+
  scale_x_continuous(labels = unit_format(unit = "M", scale = 1e-6))+
  labs(fill = "Times rated")
```


```{r errorbarplot genre performance, eval = T, echo = F, dev.args =list(bg = "azure")}
edx %>% 
  mutate(genres =str_split(genres, "\\|")) %>% 
  unnest(cols = c(genres)) %>% 
  group_by(genres) %>% 
  mutate(n = n(),avg = mean(rating),se = sd(rating)/sqrt(n))  %>%
  select(genres,rating,n,se,avg) %>% 
  distinct() %>% 
  ggplot(aes(genres,avg, ymin = avg - 2*se ,ymax =avg +2*se)) +
  geom_errorbar()+
  theme(axis.text.x = element_text(angle = 90, size = 8)) +
  theme(panel.background = element_rect(fill = "cornsilk"), plot.background = element_rect(fill = "azure", colour ="azure"))+
  ggtitle("Errorbar Plot of Genre Performance")+
  ylab("Average Rating")+
  xlab("Genres")
```


\
\
\

**Order Importance:**
\
\
To determine if there is order importance in the way the genres are stored, we quickly look at all 7 and 8 genre groups:

```{r 7 and 8 genre groups, eval = T, echo = F, message =F, warning= F, error =F}

# Looking at lone 8 genre movie

index <- edx %>% 
  mutate(genres =str_split(genres, "\\|")) %>% 
  unnest(cols = c(genres)) %>% 
  select(movieId,genres) %>% 
  group_by(movieId) %>% 
  mutate(n_genres = n_distinct(genres)) %>% 
  distinct() %>% 
  ungroup() %>% 
  select(n_genres,movieId) %>% 
  group_by(n_genres) %>% 
  filter(n_genres == 8) %>% 
  distinct() %>% 
  select(movieId) %>% 
  as.list()

edx %>% filter(movieId %in% index$movieId) %>% select(movieId,genres) %>% head()


# Looking at 7 genre movie:
index <- edx %>% 
  mutate(genres =str_split(genres, "\\|")) %>% 
  unnest(cols = c(genres)) %>% 
  select(movieId,genres) %>% 
  group_by(movieId) %>% 
  mutate(n_genres = n_distinct(genres)) %>% 
  distinct() %>% 
  ungroup() %>% 
  select(n_genres,movieId) %>% 
  group_by(n_genres) %>% 
  filter(n_genres == 7) %>% 
  distinct() %>% 
  select(movieId) %>% 
  as.list()
edx %>% filter(movieId %in% index$movieId) %>% select(movieId,genres) %>% head()

```
\

This examination shows that the order is alphabetical and thus there is no order importance present that we can analyze, and use, in future models.

\
\

## 5.5 Movie Titles
\
Exploration showed that movie titles for the same movie do not differ. Movies with the same “movieId” have the same title. We will not be concerned with changing titles, which makes the wrangling process easier.\
\
We also find additional information in the movie titles, the year the movie was published, which we will explore in more detail.

\
\

**Movie Release Years:**
\
\
Exploring the movies’ years of release, we find that the library ends before 2009. The bulk of the movies we find in the library were published past 1980, and the vast majority of movies were published after 1995. The majority of the movie ratings are also pertaining to movies that had been published in the 90’s leading up to the early 2000’s.

\
```{r movie years plot, eval = T, echo = F, dev.args =list(bg = "azure")}
edx %>% mutate(year = str_extract(title, "(\\(\\d{4}\\))")) %>%
  mutate(year = str_extract(year,"\\d{4}")) %>% 
  select(movieId,year) %>% 
  distinct() %>% 
  group_by(year) %>% 
  summarize(n = n()) %>% 
  ggplot(aes(year,n)) +
  geom_col() +
  theme(axis.text.x = element_text(angle=70,hjust = 1,size = 5)) +
  ylab("Movie Count") +
  xlab("Year") +
  theme(panel.background = element_rect(fill = "cornsilk"),
        text = element_text(family = "sans"), plot.background = element_rect(fill = "azure", colour ="azure"))

```

```{r number of ratings by movie release year, eval =T, echo = F,dev.args= list(bg= "azure")}

library(scales)

edx %>% mutate(year = str_extract(title, "(\\(\\d{4}\\))")) %>%
  mutate(year = str_extract(year,"\\d{4}")) %>% 
  select(movieId,year,userId) %>% 
  group_by(year) %>% 
  summarize(n = n()) %>% 
  ggplot(aes(year,n)) +
  geom_col() +
  theme(axis.text.x = element_text(angle=70,hjust = 1,size = 5)) +
  ylab("Ratings Count (in thousands)")+
  xlab("Movie Release Year") +
  theme(panel.background = element_rect(fill = "cornsilk"),
        text = element_text(family = "sans"), plot.background = element_rect(fill = "azure", colour ="azure"))+
  scale_y_continuous(labels = unit_format(unit = "K", scale = 1e-3))+
  ggtitle("Number of Ratings by Movie Release Year")

```
\

The splits of the movies’ release year in percent:
\

```{r distribution of movie release years, eval =T, echo = F}

edx %>% mutate(year = str_extract(title, "(\\(\\d{4}\\))")) %>%
  mutate(year = str_extract(year,"\\d{4}")) %>% 
  select(movieId,year) %>% 
  distinct() %>% 
  mutate(year= as.numeric(year)) %>% 
  group_by(year) %>% 
  summarize(n = n()) %>% 
  summarize(past_80 = mean(year >= 1980),
            between_80_95 = mean(year >=1980 & year <= 1995),
            past_95 = mean(year >= 1995)) %>% 
  kable()


```

\
The splits of ratings distribution by release year:

```{r distribution of ratings by release year, eval =T, echo = F}
edx %>% mutate(year = str_extract(title, "(\\(\\d{4}\\))")) %>%
  mutate(year = str_extract(year,"\\d{4}")) %>% 
  select(movieId,year,userId) %>% 
  mutate(year= as.numeric(year)) %>% 
  group_by(year) %>% 
  summarize(n = n()) %>% 
  summarize(between_90_03 = mean(year >=1990 & year <= 2003),
            past_90 = mean(year >= 1990)) %>% 
  kable()

```

\
Exploring the release years, we found that most movies were published in the modern era and that users prefer to rate movies that were made in modern times.
\
\
When looking at the average ratings stratified by year, we notice that older movies have a higher average rating. This could simply be due to more limited sample sizes, but they may also be partly motivated by nostalgia.

```{r average rating by year, eval =T, echo = F}
edx %>% mutate(year = str_extract(title, "(\\(\\d{4}\\))")) %>%
  mutate(year = str_extract(year,"\\d{4}")) %>% 
  select(movieId,year,userId,rating) %>% 
  mutate(year = as.numeric(year)) %>% 
  group_by(year) %>% 
  summarize(rating = mean(rating)) %>%
  select(year,rating) %>% 
  arrange(desc(rating)) %>% 
  head() %>% 
  kable()

```

```{r movie release year vs average rating plot, eval =T, echo =F, message =F, warning= F, error =F, dev.args=list(bg ="azure")}

edx %>% mutate(year = str_extract(title, "(\\(\\d{4}\\))")) %>%
  mutate(year = str_extract(year,"\\d{4}")) %>% 
  select(movieId,year,userId,rating) %>% 
  mutate(year = as.numeric(year)) %>% 
  group_by(year) %>% 
  summarize(rating = mean(rating)) %>%
  select(year,rating) %>% 
  arrange(desc(rating)) %>% 
  ggplot(aes(year,rating))+
  geom_smooth(col = "coral")+
  ylab("Rating") +
  xlab("Release Year") +
  theme(panel.background = element_rect(fill = "cornsilk"), plot.background = element_rect(fill = "azure", colour ="azure"))

```

\
\

## 5.6 Timestamps
\
Timestamps are the last feature found in our dataset that we will explore. The column “timestamps” is stored as `datetime` object and can be reshaped with `as_datetime()`.\
This feature informs us about the exact time each rating was submitted. We can extract the year, the month, the week, the day, the hour, the minute and the second.

We will stratify by years, month and hours, and subsequently plot the times against the number of ratings:

\
```{r visualizing years plot, eval = T, echo =F, message =F, warning= F, error =F, dev.args = list(bg ="azure")}
edx %>% mutate(timestamp = as_datetime(timestamp)) %>%
  mutate(timestamp = year(timestamp)) %>% 
  select(movieId,userId,timestamp) %>%
  group_by(timestamp) %>% 
  summarize(n = n()) %>% 
  ggplot(aes(timestamp,n)) +
  geom_point()+
  geom_smooth(se = F) +
  ggtitle("Number of Ratings by Submission Year")+
  ylab("Ratings Count (in thousands)")+
  xlab("Year")+
  theme(panel.background = element_rect(fill = "cornsilk"),
        text = element_text(family = "sans") ,plot.background = element_rect(fill = "azure", colour ="azure"))+
  scale_y_continuous(labels = unit_format(unit = "K", scale = 1e-3))

  
```
\

We could not find any significance in number of ratings stratified by year. There seem to be some outliers but nothing that suggests the need to further explore.
\

```{r visualizing month plot, eval =T, echo = F, message =F, warning= F, error =F, dev.args =list(bg="azure")}
edx %>% mutate(timestamp = as_datetime(timestamp)) %>%
  mutate(timestamp = month(timestamp)) %>% 
  select(movieId,userId,timestamp) %>%
  group_by(timestamp) %>% 
  summarize(n = n()) %>%
  ggplot(aes(timestamp,n)) +
  geom_point()+
  geom_smooth(method = "lm", se = F)+
  ggtitle("Number of Ratings by Submission Month")+
  ylab("Ratings Count (in thousands)")+
  xlab("Month")+
  theme(panel.background = element_rect(fill = "cornsilk"),
        text = element_text(family = "sans"), plot.background = element_rect(fill = "azure", colour ="azure"))+
  scale_y_continuous(labels = unit_format(unit = "K", scale = 1e-3))+
  scale_x_discrete(limits = 1:12,labels = c("Jan","Feb","Mar","Apr","May","Jun","Jul",
  "Aug","Oct","Nov","Sep","Dec"))

```

\
When plotting the number of ratings by month, we see a clear tendency. The number of movie ratings increases by the end of the year. More specifically, we see the last three months of the year having the highest number of ratings.\
In contrast, the month before the upswing interestingly has the least amount of ratings of any month.
\

```{r visualizing hours plot, eval =T, echo =F, message =F, warning= F, error =F,dev.args =list(bg="azure")}
edx %>% mutate(timestamp = as_datetime(timestamp)) %>%
  mutate(timestamp = hour(timestamp)) %>% 
  select(movieId,userId,timestamp) %>%
  group_by(timestamp) %>% 
  summarize(n = n()) %>%
  ggplot(aes(timestamp,n)) +
  geom_smooth(color = "coral")+
  ggtitle("Number of Ratings by Submission Hour")+
  ylab("Ratings Count (in thousands)")+
  xlab("Hour")+
  theme(panel.background = element_rect(fill = "cornsilk"),
        text = element_text(family = "sans"), plot.background = element_rect(fill = "azure", colour ="azure"))

```

\
Hours showed the clearest, most significant image of all these strata.
We see a clear slump in the morning and middle of the day, while the number of ratings gradually increases towards the evening and reaches its peak nearing 8pm. Then, it gradually and slowly falls off again. This makes sense, since most people usually watch movies in the evening, while less movies are watched during work and school hours.


Minutes and seconds were examined, although they will not be presented here. Logically, they should not provide any insights, and also showed to be insignificant in practice.

\
When we stratify again and look at the average ratings we notice that none of the strata seemed significant. A noteworthy observation was that the last three months of the year seem to have a high average rating. This could simply be due to the increased subset size.

\
```{r average rating by year plot, eval = T, echo =F, message =F, warning= F, error =F,dev.args=list(bg="azure")}
edx %>% mutate(timestamp = as_datetime(timestamp)) %>%
  mutate(timestamp = year(timestamp)) %>% 
  select(movieId,userId,timestamp,rating) %>%
  group_by(timestamp) %>% 
  summarize(rating=mean(rating), n = n()) %>% 
  ggplot(aes(timestamp,rating)) +
  geom_point()+
  xlab("Year")+
  ylab("Average Rating")+
  ggtitle("Average Rating by Year")+
  theme(panel.background = element_rect(fill = "cornsilk"),
        text = element_text(family = "sans"), plot.background = element_rect(fill = "azure", colour ="azure"))+
  annotate(geom = "text", y = 4.0,x = 1998, label = "< small sample size")

  
```


```{r average rating by month, eval =T, echo =F, message =F, warning= F, error =F,dev.args=list(bg="azure")}
edx %>% mutate(timestamp = as_datetime(timestamp)) %>%
  mutate(timestamp = month(timestamp)) %>% 
  select(movieId,userId,timestamp,rating) %>%
  group_by(timestamp) %>% 
  summarize(rating=mean(rating), n = n()) %>% 
  ggplot(aes(timestamp,rating)) +
  geom_point()+
  xlab("Month")+
  ylab("Average Rating")+
  ggtitle("Average Rating by Month")+
  theme(panel.background = element_rect(fill = "cornsilk"),plot.background = element_rect(fill = "azure", colour ="azure"))+
  scale_x_discrete(limits = 1:12,labels = c("Jan","Feb","Mar","Apr","May","Jun","Jul",
                                            "Aug","Oct","Nov","Sep","Dec"))

```


```{r average rating by hour plot, eval =T, echo =F, message =F, warning= F, error =F,dev.args=list(bg="azure")}
edx %>% mutate(timestamp = as_datetime(timestamp)) %>%
  mutate(timestamp = hour(timestamp)) %>% 
  select(movieId,userId,timestamp,rating) %>%
  group_by(timestamp) %>% 
  summarize(rating=mean(rating), n = n()) %>% 
  ggplot(aes(timestamp,rating)) +
  geom_point()+
  xlab("Hour")+
  ylab("Average Rating")+
  ggtitle("Average Rating by Hour")+
  theme(panel.background = element_rect(fill = "cornsilk"), plot.background = element_rect(fill = "azure", colour ="azure"))


```

\
\
Going forward it will be interesting to see if the findings can also be observed and confirmed by our models. We saw some significance by stratifying by month, and the hours showed to also be highly significant. In the coming analysis, we will see if that holds true in practice.


\
\
\

# 6 Models

\
Now that we have explored the underlying data on which we will train our models, familiarizing ourselves with its properties and finding possible opportunities, we can begin to build our recommendation system models. We will ultimately be comparing these to one another and find the best performing model to evaluate on the final hold out test set. For the sake of simplicity and to ease comparison, we will be saving each model’s _RMSE_ in a data frame for later use.

Our aim will be to beat the target RMSE of **0.8649**, set by the winning team of the Netflix challenge.
\

Before we begin, we write a function that lets us compute the RMSE for model evaluation:

```{r rmse function, eval =T, echo = T, message =F, warning= F, error =F}
RMSE <- function(actual_ratings, predicted_ratings){
  sqrt(mean((actual_ratings - predicted_ratings)^2))
}
```

\
\

## 6.1 Baseline Models
\
We start by establishing a few baseline models that will serve as a measure of improvement for later models:

\

### 6.1.1 Naive Models
\
The first two baseline models we will evaluate are described very well by their title. For the first one of the two “Naive Models”, we will be predicting the average (mu) for every prediction and for the second we will be guessing 3:

```{r naive model, eval =T, echo = T, message =F, warning= F, error =F, results = "hide"}

mu <- edx_train %>% summarize(mu = mean(rating)) %>% pull(mu)
mu

naive_rmse <- RMSE(edx_test$rating, mu)
naive_rmse

rmse_results <- data_frame(method = "Just the average", RMSE = naive_rmse)



naiver_rmse <- RMSE(edx_test$rating,3)

rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Guessing 3",
                                     RMSE = naiver_rmse ))
```

\

### 6.1.2 Random Guessing
\
For the third model, “random guessing” will be added to our array of baseline models. As the name suggests, we will be guessing predictions in between the range of 0.5 and 5.

```{r random guessing model, eval =T, echo =T,  message =F, warning= F, error =F, results = "hide"}
range <- seq(0.5,5,0.5)
guess_right <- function(x,y){
  mean(y==x)
}

set.seed(1)

simulation <- replicate(10000, {
  i <- sample(edx_train$rating, 1000, replace = T)
  sapply(range,guess_right,i)
})

guess_prob <- c()
for(i in 1:nrow(simulation)){
  guess_prob <- append(guess_prob,mean(simulation[i,]))
}

random_preds <- sample(range,
                       size = nrow(edx_test),
                       replace = T,
                       prob = guess_prob)

guessing_rmse <- RMSE(random_preds,edx_test$rating)

rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Random guessing",
                                     RMSE = guessing_rmse ))

```

\

### 6.1.3 Comparing Baseline Models
\
Unsurprisingly, our naive model of guessing the average performed the best so far, but it is still very far off from our target RMSE of **0.8649**. None of these models provide sufficient prediction accuracy to be used in practice, but we will use them to judge the performance of later models.


\
\
\

## 6.2 Modeling User and Movie Effects
\
As our exploratory data analysis showed, the effects the movies and users have on the data are by far the most significant features in our data. By modeling only those two effects, we should be able to predict ratings much more accurately and significantly improve our RMSE.

To incorporate these effects into our model, we will use _Linear Regression_.

It is fair to assume that these two effects will explain most of the variability in our data, and that this model can be used as a "true" baseline model going forward.

\
\

### 6.2.1 Modeling Movie Effects
\
To see how much we improve on our baseline models by just modeling one of the two main effects, we will first only analyze the effect the movies have on our predictions. We expect a highly rated movie to be considered “good”, and in turn, get rated highly by others. The reverse is true with generally negatively rated movies.

We compute the movie effects by stratifying by movie, then subtracting the average, so that all we are left with, is the difference of each movie’s average rating to the average of all movies. We will call this effect $b_i$. To make our predictions we will add the average to the extracted $b_i$ values.

```{r movie effects model, eval =T, echo =T}
mu <- mean(edx_train$rating) 
movie_avgs <- edx_train %>% 
  group_by(movieId) %>% 
  summarize(b_i = mean(rating - mu))
predicted_ratings <- mu + edx_test %>% 
  left_join(movie_avgs, by='movieId') %>%
  pull(b_i)
model_1_rmse <- RMSE(predicted_ratings, edx_test$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie Effect Model",
                                     RMSE = model_1_rmse ))
```


\

### 6.2.2 Modeling User and Movie Effects
\
We will now do the same for both, user and movie effects. Similarly to the movie effect, we expect users to generally rate higher or “nicer” when their previous ratings were high and vice versa. Each persons rating scale is different. 

We extract the user effects by stratifying by user, then subtracting the average and the previously computed movie effects. We will be left with the effect each user has on the ratings above or below the average, while discounting the movie effect. We will call the user effects $b_u$. To make our predictions we add these values together.

```{r movie and user effects model, eval =T, echo =T}
user_avgs <- edx_train %>% 
  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_i))
predicted_ratings <- edx_test %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  mutate(pred = mu + b_i + b_u) %>%
  pull(pred)
model_2_rmse <- RMSE(predicted_ratings, edx_test$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie + User Effects Model",  
                                     RMSE = model_2_rmse ))
```

\
As we can see, by just modeling the movie effects we significantly improve on the RMSE previously achieved by our baseline models. Adding the user effect to our models, we come very close to our target RMSE.

We will treat this model as our new baseline, while attempting to improve our prediction accuracy further.
\
\
\

## 6.3 Regularization
\
During our exploratory data analysis, we noticed the variability in number of ratings per movie and per user. In turn, the sample sizes we used to compute the effects in our previous model are highly inconsistent. This wide range of sample sizes can lead to uncertainty. Some movies were rated by very few users and therefore, larger estimates of $b_i$, positive or negative, are more likely. These are noisy estimates we should not trust. Large errors can increase our RMSE, so we would rather be conservative when unsure.

For that reason, we introduce weights that **penalize large estimates** formed using small sample sizes. These weights are denoted by the greek letter _lambda_ ($\lambda$) and we will use cross-validation to find the ideal lambda, that maximizes our prediction accuracy.

We call this approach _Regularization_.

As we did with our previous model, we again, will first only make use of the movie effects. Subsequently, we will progressively add more effects relevant to our data, periodically observing the effect they have on the RMSE.


\
\

### 6.3.1 Regularized Movie Effects
\
As mentioned above, the ideal value for the penalty weight must be found before continuing.

We use _Lambda-Cross Validation_ and select the lambda that minimizes the RMSE. Furthermore, we will be using the train/test sets we created earlier as to not introduce any bias.

```{r lambda cv, eval =T, echo =T}

lambdas <- seq(0, 10, 0.25)
mu <- mean(edx_train_l$rating)
just_the_sum <- edx_train_l %>% 
  group_by(movieId) %>% 
  summarize(s = sum(rating - mu), n_i = n())
rmses <- sapply(lambdas, function(l){
  predicted_ratings <- edx_test_l %>% 
    left_join(just_the_sum, by='movieId') %>% 
    mutate(b_i = s/(n_i+l)) %>%
    mutate(pred = mu + b_i) %>%
    pull(pred)
  return(RMSE(predicted_ratings, edx_test_l$rating))
})

```

\
As we can see here, the lambda that minimizes the RMSE, and therefore, the lambda we will be using, is `r lambdas[which.min(rmses)]` : 

\
```{r lambda cv plot, eval = T, echo = F,dev.args=list(bg="azure")}
library(gghighlight)
ggplot(data =data.frame(rmses=rmses,lambdas =lambdas),
       aes(x = lambdas, y = rmses))+
  geom_point(col ="coral")+
  theme(panel.background = element_rect(fill = "cornsilk"), plot.background = element_rect(fill = "azure", colour ="azure"))+
  xlab("Lambdas")+
  ylab("Rmse")+
  gghighlight(rmses == min(rmses))
```

\
It is worth mentioning that the distinction between using the actual sets, using only one test/train set and the method adopted here with two test/train sets to determine the lambda in regularization is insignificant, and they all produce the same outcome. We opted to divide the dataset twice to closely replicate a scenario where unidentified ratings have to be anticipated. 

\

We can now insert the selected lambda into our model and make predictions:

```{r regularized movie effects model, eval =T, echo =T}

lambda <- lambdas[which.min(rmses)]
movie_reg_avgs <- edx_train %>% 
  group_by(movieId) %>% 
  summarize(b_i = sum(rating - mu)/(n()+lambda), n_i = n()) 

predicted_ratings <- edx_test %>% 
  left_join(movie_reg_avgs, by='movieId') %>%
  mutate(pred = mu + b_i) %>%
  pull(pred)

model_3_rmse <- RMSE(predicted_ratings, edx_test$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Regularized Movie Effect Model",  
                                     RMSE = model_3_rmse ))
```


\
\

### 6.3.2 Regularized Movie and User Effects
\
We are now ready to add the user effect to our regularized model. We will take the same steps as in our first regularized model and select the lambda based on cross-validation:

```{r reg movie and user effect model, eval =T, echo =T, message =F, warning= F, error =F, results = "hide"}
regularization <- function(lambda,edx_train_l,edx_test_l){
  mu <- mean(edx_train_l$rating)
  b_i <- edx_train_l %>% 
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+lambda))
  b_u <- edx_train_l %>% 
    left_join(b_i, by="movieId") %>%
    filter(!is.na(b_i)) %>% 
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+lambda))
  predicted_ratings <- 
    edx_test_l %>% 
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    filter(!is.na(b_i), !is.na(b_u)) %>% 
    mutate(pred = mu + b_i + b_u) %>%
    pull(pred)
  return(RMSE(predicted_ratings, edx_test_l$rating))
}


lambdas <- seq(0,10,0.25)

lambdas_rmse <- sapply(lambdas,
                       regularization,
                       edx_train_l = edx_train_l,
                       edx_test_l = edx_test_l)

lambdas_tibble <- tibble(Lambda = lambdas, RMSE = lambdas_rmse)

lambdas_tibble


lambda <- lambdas[which.min(lambdas_rmse)]


mu <- mean(edx_train$rating)

b_i_reg <- edx_train %>% 
  group_by(movieId) %>%
  summarize(b_i = sum(rating - mu)/(n()+lambda))

b_u_reg <- edx_train %>% 
  left_join(b_i_reg, by="movieId") %>%
  group_by(userId) %>%
  summarize(b_u = sum(rating - b_i - mu)/(n()+lambda))


preds <- edx_test %>% 
  left_join(b_i_reg, by = "movieId") %>% 
  left_join(b_u_reg, by = "userId") %>% 
  mutate(prediction = mu + b_i +b_u) %>% 
  pull(prediction)

reg_model_rmse <- RMSE(preds, edx_test$rating)


rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Regularized Movie Effect + User Effect Model",  
                                     RMSE = reg_model_rmse ))


```

\
We were able to improve our model's RMSE by using regularization, and it now stands at a value of `r reg_model_rmse`. Surprisingly, we were able to achieve our target RMSE using only two regularized effects.  
To further minimize the RMSE, we will progressively add more features that are statistically significant based on our data exploration.


\
\

### 6.3.3 Regularized Genre Effects
\
All the movies in our dataset, except for one, have been assigned at least one genre. However, it is likely that certain genres will be more popular than others. Since most movies have multiple genres assigned to them, we need to decide how to handle this information. We can either separate the different genres and calculate their individual effects, or we can treat the group of genres assigned to each movie as a single unit.

We will examine both approaches, beginning with dividing the genre groups into individual genres. This will result in multiple genre effects for movies that are categorized into multiple genres. In order to assign each movie and its corresponding rating a single genre effect, we first calculate each genre's effect separately. After that, we average the effects by movie to assign each movie a unique genre effect.

We will denote the genre effect with $b_g$.

```{r reg genre effect, eval = T, echo = T, message =F, warning= F, error =F, results = "hide"}
regularization <- function(lambda,edx_train_l,edx_test_l){
  mu <- mean(edx_train_l$rating)
  b_i <- edx_train_l %>% 
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+lambda))
  b_u <- edx_train_l %>% 
    left_join(b_i, by="movieId") %>%
    filter(!is.na(b_i)) %>% 
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+lambda))
  b_g <-edx_train_l %>% 
    left_join(b_i, by = "movieId") %>% 
    left_join(b_u, by = "userId") %>% 
    filter(!is.na(b_i), !is.na(b_u)) %>% 
    mutate(genres =str_split(genres, "\\|")) %>% 
    unnest(cols = c(genres)) %>% 
    group_by(genres) %>% 
    mutate(b_g = sum(rating- mu - b_i - b_u)/(n()+lambda)) %>% 
    ungroup() %>% 
    group_by(movieId) %>% 
    summarize(b_g = mean(b_g))
  predicted_ratings <- 
    edx_test_l %>% 
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    left_join(b_g, by = "movieId") %>% 
    filter(!is.na(b_i), !is.na(b_u),!is.na(b_g)) %>% 
    mutate(pred = mu + b_i + b_u +b_g) %>%
    pull(pred)
  return(RMSE(predicted_ratings, edx_test_l$rating))
}


lambdas <- seq(0,10,0.25)

lambdas_rmse <- sapply(lambdas,
                       regularization,
                       edx_train_l = edx_train_l,
                       edx_test_l = edx_test_l)

lambdas_tibble <- tibble(Lambda = lambdas, RMSE = lambdas_rmse)

lambdas_tibble



lambda <- lambdas[which.min(lambdas_rmse)]


mu <- mean(edx_train$rating)

b_i_reg <- edx_train %>% 
  group_by(movieId) %>%
  summarize(b_i = sum(rating - mu)/(n()+lambda))

b_u_reg <- edx_train %>% 
  left_join(b_i_reg, by="movieId") %>%
  group_by(userId) %>%
  summarize(b_u = sum(rating - b_i - mu)/(n()+lambda))

b_g_reg <- edx_train %>% 
  left_join(b_i_reg, by="movieId") %>% 
  left_join(b_u_reg, by="userId") %>% 
  mutate(genres =str_split(genres, "\\|")) %>% 
  unnest(cols = c(genres)) %>% 
  group_by(genres) %>% 
  mutate(b_g = sum(rating- mu - b_i - b_u)/(n()+lambda)) %>% 
  ungroup() %>% 
  group_by(movieId) %>% 
  summarize(b_g = mean(b_g))

preds <- edx_test %>% 
  left_join(b_i_reg, by = "movieId") %>% 
  left_join(b_u_reg, by = "userId") %>% 
  left_join(b_g_reg, by = "movieId") %>% 
  mutate(prediction = mu + b_i +b_u+b_g) %>% 
  pull(prediction)

reg_model_2_rmse <- RMSE(preds, edx_test$rating)


rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Regularized with split Genre Effects Model",  
                                     RMSE = reg_model_2_rmse ))

```



\
\

### 6.3.4 Regularized Joint Genres Effects
\
For our second approach to dealing with the observed genre effect, we will leave the genre column of the dataset untouched. We treat each collection of genres assigned to a particular movie as its own genre and compare the results to our previous approach:

```{r reg joint genres, eval =T, echo = T, message =F, warning= F, error =F, results = "hide"}
regularization <- function(lambda,edx_train_l,edx_test_l){
  mu <- mean(edx_train_l$rating)
  b_i <- edx_train_l %>% 
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+lambda))
  b_u <- edx_train_l %>% 
    left_join(b_i, by="movieId") %>%
    filter(!is.na(b_i)) %>% 
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+lambda))
  b_g <-edx_train_l %>% 
    left_join(b_i, by = "movieId") %>% 
    left_join(b_u, by = "userId") %>% 
    filter(!is.na(b_i), !is.na(b_u)) %>% 
    group_by(genres) %>% 
    summarize(b_g = sum(rating -b_i-b_u-mu)/(n()+lambda))
  predicted_ratings <- 
    edx_test_l %>% 
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    left_join(b_g, by = "genres") %>% 
  filter(!is.na(b_i), !is.na(b_u),!is.na(b_g)) %>% 
    mutate(pred = mu + b_i + b_u +b_g) %>%
    pull(pred)
  return(RMSE(predicted_ratings, edx_test_l$rating))
}


lambdas <- seq(0,10,0.25)

lambdas_rmse <- sapply(lambdas,
                       regularization,
                       edx_train_l = edx_train_l,
                       edx_test_l = edx_test_l)

lambdas_tibble <- tibble(Lambda = lambdas, RMSE = lambdas_rmse)

lambdas_tibble



lambda <- lambdas[which.min(lambdas_rmse)]


mu <- mean(edx_train$rating)

b_i_reg <- edx_train %>% 
  group_by(movieId) %>%
  summarize(b_i = sum(rating - mu)/(n()+lambda))

b_u_reg <- edx_train %>% 
  left_join(b_i_reg, by="movieId") %>%
  group_by(userId) %>%
  summarize(b_u = sum(rating - b_i - mu)/(n()+lambda))

b_g_reg <- edx_train %>% 
  left_join(b_i_reg, by="movieId") %>% 
  left_join(b_u_reg, by="userId") %>% 
  group_by(genres) %>% 
  summarize(b_g = sum(rating -b_i-b_u-mu)/(n()+lambda))

preds <- edx_test %>% 
  left_join(b_i_reg, by = "movieId") %>% 
  left_join(b_u_reg, by = "userId") %>% 
  left_join(b_g_reg, by = "genres") %>% 
mutate(prediction = mu + b_i +b_u+b_g) %>% 
  pull(prediction)

reg_model_3_rmse <- RMSE(preds, edx_test$rating)

rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Regularized with joint Genre Effect Model",  
                                     RMSE = reg_model_3_rmse ))

```

\
Leaving the genres untouched surprisingly yields slightly better results. This may be due to the specificity of genre groups, which create their own classification when used together.

Going forward we will use the “Joint Genre Approach” to estimate the genre effect.



\
\

### 6.3.5 Regularized Time Effects
\
We will now explore the relation between time and ratings. As discussed during our exploratory data analysis, our data includes `datetime` objects for each rating, as well as the release years for each rated film. Upon observing the data, we found enough evidence to suggest that ratings can be influenced by when they are given.

The datetime assigned to each rating allows us to extract the exact year, month, week, day, hour, minute and second of submission, that we in turn, can use as additional separate effects.

We will focus on only the year, month, week and hour effect, since all other time effects have proven to be far too insignificant.

In contrast to all factors discussed thus far, these effects will be the first _non-categorical_ effects to be implemented into our model. To avoid overfitting, we could consider using methods to smooth the data. However, we chose not to do so in this project because such methods would not have a significant impact on our results and are not worth exploring within the scope of this project.



\
\

#### Regularized "Years Between" Effects
\
Before exploring the unambiguous time effects mentioned above, we will investigate a more imaginative assumption. We will assume that the time between a movie’s release and the time of rating holds significance. A movie that was released long before the user’s respective rating might generally be received more positively, motivated by nostalgia. Some movies may also be considered iconic, regardless of their quality, which may only manifest years after release. Rewatching a particular movie multiple times, strengthening ones opinion of it, might also be a factor. On the other hand, a movie rated close to its release might capture the zeitgeist and get received positively due to that. It is possible that both of these cases may also be true if we were to reverse them.

We will call this theoretical effect “Years Between” and denote it with $b_t$.

```{r years between model, eval =T, echo =T, message =F, warning= F, error =F, results = "hide"}
regularization <- function(lambda,edx_train_l,edx_test_l){
  mu <- mean(edx_train_l$rating)
  b_i <- edx_train_l %>% 
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+lambda))
  b_u <- edx_train_l %>% 
    left_join(b_i, by="movieId") %>%
    filter(!is.na(b_i)) %>% 
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+lambda))
  b_g <-edx_train_l %>% 
    left_join(b_i, by = "movieId") %>% 
    left_join(b_u, by = "userId") %>% 
    filter(!is.na(b_i), !is.na(b_u)) %>% 
    group_by(genres) %>% 
    summarize(b_g = sum(rating -b_i-b_u-mu)/(n()+lambda))
  b_t <- edx_train_l %>% 
    left_join(b_i, by = "movieId") %>% 
    left_join(b_u, by = "userId") %>%
    left_join(b_g, by = "genres") %>% 
    mutate(year = str_extract(title, "(\\(\\d{4}\\))"),
           title = str_replace(title,"(\\(\\d{4}\\))$","")) %>%
    mutate(year = as.numeric(str_extract(year,"\\d{4}"))) %>% 
    mutate(timestamp = as_datetime(timestamp)) %>%
    mutate(timestamp = year(timestamp)) %>% 
    mutate(years_between = timestamp - year) %>% 
    group_by(years_between) %>% 
    summarize(b_t = sum(rating -mu-b_i-b_u-b_g)/(n()+lambda))

  predicted_ratings <- 
    edx_test_l %>% 
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    left_join(b_g, by = "genres") %>%
    mutate(year = str_extract(title, "(\\(\\d{4}\\))"),
           title = str_replace(title,"(\\(\\d{4}\\))$","")) %>%
    mutate(year = as.numeric(str_extract(year,"\\d{4}"))) %>% 
    mutate(timestamp = as_datetime(timestamp)) %>%
    mutate(timestamp = year(timestamp)) %>% 
    mutate(years_between = timestamp - year) %>% 
    left_join(b_t, by ="years_between") %>% 
  filter(!is.na(b_i), !is.na(b_u),!is.na(b_u),!is.na(b_t)) %>% 
    mutate(pred = mu + b_i + b_u +b_g+b_t) %>%
    pull(pred)
  return(RMSE(predicted_ratings, edx_test_l$rating))
}


lambdas <- seq(0,10,0.25)

lambdas_rmse <- sapply(lambdas,
                       regularization,
                       edx_train_l = edx_train_l,
                       edx_test_l = edx_test_l)

lambdas_tibble <- tibble(Lambda = lambdas, RMSE = lambdas_rmse)

lambdas_tibble



lambda <- lambdas[which.min(lambdas_rmse)]


mu <- mean(edx_train$rating)

b_i_reg <- edx_train %>% 
  group_by(movieId) %>%
  summarize(b_i = sum(rating - mu)/(n()+lambda))

b_u_reg <- edx_train %>% 
  left_join(b_i_reg, by="movieId") %>%
  group_by(userId) %>%
  summarize(b_u = sum(rating - b_i - mu)/(n()+lambda))

b_g_reg <- edx_train %>% 
  left_join(b_i_reg, by="movieId") %>% 
  left_join(b_u_reg, by="userId") %>% 
  group_by(genres) %>% 
  summarize(b_g = sum(rating -b_i-mu)/(n()+lambda))

b_t_reg <- edx_train %>% 
  left_join(b_i_reg, by="movieId") %>% 
  left_join(b_u_reg, by="userId") %>%
  left_join(b_g_reg, by = "genres") %>%
  mutate(year = str_extract(title, "(\\(\\d{4}\\))"),
         title = str_replace(title,"(\\(\\d{4}\\))$","")) %>%
  mutate(year = as.numeric(str_extract(year,"\\d{4}"))) %>% 
  mutate(timestamp = as_datetime(timestamp)) %>%
  mutate(timestamp = year(timestamp)) %>% 
  mutate(years_between = timestamp - year) %>% 
  group_by(years_between) %>% 
  summarize(b_t = sum(rating -mu-b_i-b_u-b_g)/(n()+lambda))


preds <- edx_test %>% 
  left_join(b_i_reg, by = "movieId") %>% 
  left_join(b_u_reg, by = "userId") %>% 
  left_join(b_g_reg, by = "genres") %>% 
  mutate(year = str_extract(title, "(\\(\\d{4}\\))"),
         title = str_replace(title,"(\\(\\d{4}\\))$","")) %>%
  mutate(year = as.numeric(str_extract(year,"\\d{4}"))) %>% 
  mutate(timestamp = as_datetime(timestamp)) %>%
  mutate(timestamp = year(timestamp)) %>% 
  mutate(years_between = timestamp - year) %>% 
  left_join(b_t_reg, by ="years_between") %>% 
  mutate(prediction = mu + b_i +b_u+b_g+b_t) %>% 
  pull(prediction)

reg_model_4_rmse <- RMSE(preds, edx_test$rating)

rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Regularized with \"years between\" Effect Model",  
                                     RMSE = reg_model_4_rmse ))

```

\
The results showed that our assumption was incorrect. In fact, this effect had a negative impact on our results when compared to the previous model that did not account for it.

The worsened performance could be attributed to the fact that the factors assumed above, cancel each other out, or do not occur frequently enough to be predictive. Instead of adding predictive value, only more noise was added.



\
\

#### Regularized Hour Effects
\
The first datetime we extract to treat as a unique effect will be the hour at which the rating was submitted. Exploring the volume of ratings throughout the day, we observed a gradual increase in rating volume, peaking around the evening, before falling off again. This observation alone does not suggest predictive value. \
Nevertheless, we will analyze this time effect and denote it by a lowercase "b" along with its respective lowercase initial letter, in an attempt to improve our prediction accuracy.

```{r reg hour effects, eval =T, echo =T, message =F, warning= F, error =F, results = "hide"}

regularization <- function(lambda,edx_train_l,edx_test_l){
  mu <- mean(edx_train_l$rating)
  b_i <- edx_train_l %>% 
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+lambda))
  b_u <- edx_train_l %>% 
    left_join(b_i, by="movieId") %>%
    filter(!is.na(b_i)) %>% 
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+lambda))
  b_g <-edx_train_l %>% 
    left_join(b_i, by = "movieId") %>% 
    left_join(b_u, by = "userId") %>% 
    filter(!is.na(b_i), !is.na(b_u)) %>% 
    group_by(genres) %>% 
    summarize(b_g = sum(rating -b_i-b_u-mu)/(n()+lambda))
  b_h <- edx_train_l %>% 
    left_join(b_i, by = "movieId") %>% 
    left_join(b_u, by = "userId") %>% 
    left_join(b_g, by = "genres") %>% 
    mutate(timestamp = as_datetime(timestamp)) %>%
    mutate(timestamp = hour(timestamp)) %>% 
    group_by(timestamp) %>% 
    summarize(b_h = sum(rating -b_i-b_u-b_g-mu)/(n()+lambda))
    
  predicted_ratings <- 
    edx_test_l %>% 
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    left_join(b_g, by = "genres") %>% 
    mutate(timestamp = as_datetime(timestamp)) %>%
    mutate(timestamp = hour(timestamp)) %>% 
    left_join(b_h, by = "timestamp") %>% 
    filter(!is.na(b_i), !is.na(b_u),!is.na(b_g),!is.na(b_h)) %>% 
    mutate(pred = mu + b_i + b_u +b_g+b_h) %>%
    pull(pred)
  return(RMSE(predicted_ratings, edx_test_l$rating))
}


lambdas <- seq(0,10,0.25)

lambdas_rmse <- sapply(lambdas,
                       regularization,
                       edx_train_l = edx_train_l,
                       edx_test_l = edx_test_l)

lambdas_tibble <- tibble(Lambda = lambdas, RMSE = lambdas_rmse)

lambdas_tibble



lambda <- lambdas[which.min(lambdas_rmse)]


mu <- mean(edx_train$rating)

b_i_reg <- edx_train %>% 
  group_by(movieId) %>%
  summarize(b_i = sum(rating - mu)/(n()+lambda))

b_u_reg <- edx_train %>% 
  left_join(b_i_reg, by="movieId") %>%
  group_by(userId) %>%
  summarize(b_u = sum(rating - b_i - mu)/(n()+lambda))

b_g_reg <- edx_train %>% 
  left_join(b_i_reg, by="movieId") %>% 
  left_join(b_u_reg, by="userId") %>% 
  group_by(genres) %>% 
  summarize(b_g = sum(rating -b_i-b_u-mu)/(n()+lambda))

b_h_reg <- edx_train %>% 
  left_join(b_i_reg, by="movieId") %>% 
  left_join(b_u_reg, by="userId") %>% 
  left_join(b_g_reg, by ="genres") %>% 
  mutate(timestamp = as_datetime(timestamp)) %>%
  mutate(timestamp = hour(timestamp)) %>% 
  group_by(timestamp) %>% 
  summarize(b_h = sum(rating -b_i-b_u-b_g-mu)/(n()+lambda))


preds <- edx_test %>% 
  left_join(b_i_reg, by = "movieId") %>% 
  left_join(b_u_reg, by = "userId") %>% 
  left_join(b_g_reg, by = "genres") %>%
  mutate(timestamp = as_datetime(timestamp)) %>%
  mutate(timestamp = hour(timestamp)) %>% 
  left_join(b_h_reg, by = "timestamp") %>% 
  mutate(prediction = mu + b_i +b_u+b_g+b_h) %>% 
  pull(prediction)

reg_model_5_rmse <- RMSE(preds, edx_test$rating)


rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Regularized with Hour Effect Model",  
                                     RMSE = reg_model_5_rmse ))

```

\
The error was slightly improved upon, proving the submission hour effect to be a useful additive to the model. 


\
\

#### Regularized Year Effects
\
We will now explore the effect that the rating submission year has on the ratings. Our implementation process stays the same, we stratify by year, and sweep all other observed effects.

```{r reg year effects, eval =T, echo =T, message =F, warning= F, error =F, results = "hide"}
regularization <- function(lambda,edx_train_l,edx_test_l){
  mu <- mean(edx_train_l$rating)
  b_i <- edx_train_l %>% 
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+lambda))
  b_u <- edx_train_l %>% 
    left_join(b_i, by="movieId") %>%
    filter(!is.na(b_i)) %>% 
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+lambda))
  b_g <-edx_train_l %>% 
    left_join(b_i, by = "movieId") %>% 
    left_join(b_u, by = "userId") %>% 
    filter(!is.na(b_i), !is.na(b_u)) %>% 
    group_by(genres) %>% 
    summarize(b_g = sum(rating -b_i-b_u-mu)/(n()+lambda))
  b_y <- edx_train_l %>% 
    left_join(b_i, by = "movieId") %>% 
    left_join(b_u, by = "userId") %>% 
    left_join(b_g, by = "genres") %>% 
    mutate(timestamp = as_datetime(timestamp)) %>%
    mutate(timestamp = year(timestamp)) %>% 
    group_by(timestamp) %>% 
    summarize(b_y = sum(rating -b_i-b_u-b_g-mu)/(n()+lambda))
  
  predicted_ratings <- 
    edx_test_l %>% 
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    left_join(b_g, by = "genres") %>% 
    mutate(timestamp = as_datetime(timestamp)) %>%
    mutate(timestamp = year(timestamp)) %>% 
    left_join(b_y, by = "timestamp") %>% 
    filter(!is.na(b_i), !is.na(b_u),!is.na(b_g),!is.na(b_y)) %>% 
    mutate(pred = mu + b_i + b_u +b_g+b_y) %>%
    pull(pred)
  return(RMSE(predicted_ratings, edx_test_l$rating))
}


lambdas <- seq(0,10,0.25)

lambdas_rmse <- sapply(lambdas,
                       regularization,
                       edx_train_l = edx_train_l,
                       edx_test_l = edx_test_l)

lambdas_tibble <- tibble(Lambda = lambdas, RMSE = lambdas_rmse)

lambdas_tibble



lambda <- lambdas[which.min(lambdas_rmse)]


mu <- mean(edx_train$rating)

b_i_reg <- edx_train %>% 
  group_by(movieId) %>%
  summarize(b_i = sum(rating - mu)/(n()+lambda))

b_u_reg <- edx_train %>% 
  left_join(b_i_reg, by="movieId") %>%
  group_by(userId) %>%
  summarize(b_u = sum(rating - b_i - mu)/(n()+lambda))

b_g_reg <- edx_train %>% 
  left_join(b_i_reg, by="movieId") %>% 
  left_join(b_u_reg, by="userId") %>% 
  group_by(genres) %>% 
  summarize(b_g = sum(rating -b_i-b_u-mu)/(n()+lambda))

b_y_reg <- edx_train %>% 
  left_join(b_i_reg, by="movieId") %>% 
  left_join(b_u_reg, by="userId") %>% 
  left_join(b_g_reg, by ="genres") %>% 
  mutate(timestamp = as_datetime(timestamp)) %>%
  mutate(timestamp = year(timestamp)) %>% 
  group_by(timestamp) %>% 
  summarize(b_y = sum(rating -b_i-b_u-b_g-mu)/(n()+lambda))


preds <- edx_test %>% 
  left_join(b_i_reg, by = "movieId") %>% 
  left_join(b_u_reg, by = "userId") %>% 
  left_join(b_g_reg, by = "genres") %>%
  mutate(timestamp = as_datetime(timestamp)) %>%
  mutate(timestamp = year(timestamp)) %>% 
  left_join(b_y_reg, by = "timestamp") %>% 
  mutate(prediction = mu + b_i +b_u+b_g+b_y) %>% 
  pull(prediction)

reg_model_6_rmse <- RMSE(preds, edx_test$rating)


rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Regularized with Year Effect Model",  
                                     RMSE = reg_model_6_rmse ))

```

\
We have observed a minor improvement in the RMSE compared to the hour effect, making it the best performing "time effect" thus far. To that extent, it is also the overall best performing model tested to this point.



\
\

#### Regularized Month Effects
\
While exploring the data, we noticed certain months having a higher average rating. That being said, that observation alone does not allow us to make assumptions on its significance as model feature. 

We will incorporate the month effect to explore its value to our model while comparing it to the already discussed time effects. We will replicate our approach from above and apply it to the month effect.

```{r reg month effect, eval =T, echo = T, message =F, warning= F, error =F, results = "hide"}
regularization <- function(lambda,edx_train_l,edx_test_l){
  mu <- mean(edx_train_l$rating)
  b_i <- edx_train_l %>% 
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+lambda))
  b_u <- edx_train_l %>% 
    left_join(b_i, by="movieId") %>%
    filter(!is.na(b_i)) %>% 
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+lambda))
  b_g <-edx_train_l %>% 
    left_join(b_i, by = "movieId") %>% 
    left_join(b_u, by = "userId") %>% 
    filter(!is.na(b_i), !is.na(b_u)) %>% 
    group_by(genres) %>% 
    summarize(b_g = sum(rating -b_i-b_u-mu)/(n()+lambda))
  b_m <- edx_train_l %>% 
    left_join(b_i, by = "movieId") %>% 
    left_join(b_u, by = "userId") %>% 
    left_join(b_g, by = "genres") %>% 
    mutate(timestamp = as_datetime(timestamp)) %>%
    mutate(timestamp = month(timestamp)) %>% 
    group_by(timestamp) %>% 
    summarize(b_m = sum(rating -b_i-b_u-b_g-mu)/(n()+lambda))
  
  predicted_ratings <- 
    edx_test_l %>% 
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    left_join(b_g, by = "genres") %>% 
    mutate(timestamp = as_datetime(timestamp)) %>%
    mutate(timestamp = month(timestamp)) %>% 
    left_join(b_m, by = "timestamp") %>% 
    filter(!is.na(b_i), !is.na(b_u),!is.na(b_g),!is.na(b_m)) %>% 
    mutate(pred = mu + b_i + b_u +b_g+b_m) %>%
    pull(pred)
  return(RMSE(predicted_ratings, edx_test_l$rating))
}


lambdas <- seq(0,10,0.25)

lambdas_rmse <- sapply(lambdas,
                       regularization,
                       edx_train_l = edx_train_l,
                       edx_test_l = edx_test_l)

lambdas_tibble <- tibble(Lambda = lambdas, RMSE = lambdas_rmse)

lambdas_tibble



lambda <- lambdas[which.min(lambdas_rmse)]


mu <- mean(edx_train$rating)

b_i_reg <- edx_train %>% 
  group_by(movieId) %>%
  summarize(b_i = sum(rating - mu)/(n()+lambda))

b_u_reg <- edx_train %>% 
  left_join(b_i_reg, by="movieId") %>%
  group_by(userId) %>%
  summarize(b_u = sum(rating - b_i - mu)/(n()+lambda))

b_g_reg <- edx_train %>% 
  left_join(b_i_reg, by="movieId") %>% 
  left_join(b_u_reg, by="userId") %>% 
  group_by(genres) %>% 
  summarize(b_g = sum(rating -b_i-b_u-mu)/(n()+lambda))

b_m_reg <- edx_train %>% 
  left_join(b_i_reg, by="movieId") %>% 
  left_join(b_u_reg, by="userId") %>% 
  left_join(b_g_reg, by ="genres") %>% 
  mutate(timestamp = as_datetime(timestamp)) %>%
  mutate(timestamp = month(timestamp)) %>% 
  group_by(timestamp) %>% 
  summarize(b_m = sum(rating -b_i-b_u-b_g-mu)/(n()+lambda))


preds <- edx_test %>% 
  left_join(b_i_reg, by = "movieId") %>% 
  left_join(b_u_reg, by = "userId") %>% 
  left_join(b_g_reg, by = "genres") %>%
  mutate(timestamp = as_datetime(timestamp)) %>%
  mutate(timestamp = month(timestamp)) %>% 
  left_join(b_m_reg, by = "timestamp") %>% 
  mutate(prediction = mu + b_i +b_u+b_g+b_m) %>% 
  pull(prediction)

reg_model_7_rmse <- RMSE(preds, edx_test$rating)


rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Regularized with Month Effect Model",  
                                     RMSE = reg_model_7_rmse ))

```

\
This model performs slightly better than its iteration with the hour effect while demonstrating less predictive power than its year equivalent. 



\
\

#### Regularized Week Effects
\
The last “time effect” we will analyze is the week effect. Yet again, we will be exploring if this time factor adds value to our model by comparing it to its predecessors. We will replicate our previously used approach and apply it to the week effect.

```{r reg week effects, eval =T, echo =T, message =F, warning= F, error =F, results = "hide"}
regularization <- function(lambda,edx_train_l,edx_test_l){
  mu <- mean(edx_train_l$rating)
  b_i <- edx_train_l %>% 
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+lambda))
  b_u <- edx_train_l %>% 
    left_join(b_i, by="movieId") %>%
    filter(!is.na(b_i)) %>% 
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+lambda))
  b_g <-edx_train_l %>% 
    left_join(b_i, by = "movieId") %>% 
    left_join(b_u, by = "userId") %>% 
    filter(!is.na(b_i), !is.na(b_u)) %>% 
    group_by(genres) %>% 
    summarize(b_g = sum(rating -b_i-b_u-mu)/(n()+lambda))
  b_w <- edx_train_l %>% 
    left_join(b_i, by = "movieId") %>% 
    left_join(b_u, by = "userId") %>% 
    left_join(b_g, by = "genres") %>% 
    mutate(timestamp = as_datetime(timestamp)) %>%
    mutate(timestamp = week(timestamp)) %>% 
    group_by(timestamp) %>% 
    summarize(b_w = sum(rating -b_i-b_u-b_g-mu)/(n()+lambda))
  
  predicted_ratings <- 
    edx_test_l %>% 
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    left_join(b_g, by = "genres") %>% 
    mutate(timestamp = as_datetime(timestamp)) %>%
    mutate(timestamp = week(timestamp)) %>% 
    left_join(b_w, by = "timestamp") %>% 
    filter(!is.na(b_i), !is.na(b_u),!is.na(b_g),!is.na(b_w)) %>% 
    mutate(pred = mu + b_i + b_u +b_g+b_w) %>%
    pull(pred)
  return(RMSE(predicted_ratings, edx_test_l$rating))
}


lambdas <- seq(0,10,0.25)

lambdas_rmse <- sapply(lambdas,
                       regularization,
                       edx_train_l = edx_train_l,
                       edx_test_l = edx_test_l)

lambdas_tibble <- tibble(Lambda = lambdas, RMSE = lambdas_rmse)

lambdas_tibble



lambda <- lambdas[which.min(lambdas_rmse)]


mu <- mean(edx_train$rating)

b_i_reg <- edx_train %>% 
  group_by(movieId) %>%
  summarize(b_i = sum(rating - mu)/(n()+lambda))

b_u_reg <- edx_train %>% 
  left_join(b_i_reg, by="movieId") %>%
  group_by(userId) %>%
  summarize(b_u = sum(rating - b_i - mu)/(n()+lambda))

b_g_reg <- edx_train %>% 
  left_join(b_i_reg, by="movieId") %>% 
  left_join(b_u_reg, by="userId") %>% 
  group_by(genres) %>% 
  summarize(b_g = sum(rating -b_i-b_u-mu)/(n()+lambda))

b_w_reg <- edx_train %>% 
  left_join(b_i_reg, by="movieId") %>% 
  left_join(b_u_reg, by="userId") %>% 
  left_join(b_g_reg, by ="genres") %>% 
  mutate(timestamp = as_datetime(timestamp)) %>%
  mutate(timestamp = week(timestamp)) %>% 
  group_by(timestamp) %>% 
  summarize(b_w = sum(rating -b_i-b_u-b_g-mu)/(n()+lambda))


preds <- edx_test %>% 
  left_join(b_i_reg, by = "movieId") %>% 
  left_join(b_u_reg, by = "userId") %>% 
  left_join(b_g_reg, by = "genres") %>%
  mutate(timestamp = as_datetime(timestamp)) %>%
  mutate(timestamp = week(timestamp)) %>% 
  left_join(b_w_reg, by = "timestamp") %>% 
  mutate(prediction = mu + b_i +b_u+b_g+b_w) %>% 
  pull(prediction)

reg_model_8_rmse <- RMSE(preds, edx_test$rating)


rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Regularized with Weeks Effect Model",  
                                     RMSE = reg_model_8_rmse ))

```

\
Using the week effect as our lone “time effect” we obtain the second lowest RMSE thus far, proving the week effect to be a beneficial addition to our model.



\
\

### 6.3.6 Final Regularized Model
\
To construct a final _Regularized Linear Regression Model_ we include all “time effects” that showed significant improvement to our prediction error.

This model now includes the following:

- Movie Effect

- User Effect

- Genre Effect (“Joint Genres”/ untouched)

- Year Effect

- Month Effect

- Week Effect

- Hour Effect

\

We first cross-validate for the lambda value that minimizes the RMSE:

```{r final reg cv, eval = T, echo =T, class.source ="fold-hide", message =F, warning= F, error =F, results = "hide"}

regularization <- function(lambda,edx_train_l,edx_test_l){
  mu <- mean(edx_train_l$rating)
  b_i <- edx_train_l %>% 
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+lambda))
  b_u <- edx_train_l %>% 
    left_join(b_i, by="movieId") %>%
    filter(!is.na(b_i)) %>% 
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+lambda))
  b_g <-edx_train_l %>% 
    left_join(b_i, by = "movieId") %>% 
    left_join(b_u, by = "userId") %>% 
    filter(!is.na(b_i), !is.na(b_u)) %>% 
    group_by(genres) %>% 
    summarize(b_g = sum(rating -b_i-b_u-mu)/(n()+lambda))
  b_y <- edx_train_l %>% 
    left_join(b_i, by = "movieId") %>% 
    left_join(b_u, by = "userId") %>% 
    left_join(b_g, by = "genres") %>% 
    mutate(timestamp = as_datetime(timestamp)) %>%
    mutate(year = year(timestamp)) %>% 
    group_by(year) %>% 
    summarize(b_y = sum(rating -b_i-b_u-b_g-mu)/(n()+lambda))
  b_m <- edx_train_l %>% 
    left_join(b_i, by = "movieId") %>% 
    left_join(b_u, by = "userId") %>% 
    left_join(b_g, by = "genres") %>% 
    mutate(timestamp = as_datetime(timestamp)) %>%
    mutate(year = year(timestamp)) %>% 
    left_join(b_y, by = "year") %>% 
    mutate(timestamp = as_datetime(timestamp)) %>%
    mutate(month = month(timestamp)) %>%
    group_by(month) %>% 
    summarize(b_m = sum(rating -b_i-b_u-b_g-b_y-mu)/(n()+lambda))
  b_w <-edx_train_l %>% 
    left_join(b_i, by = "movieId") %>% 
    left_join(b_u, by = "userId") %>% 
    left_join(b_g, by = "genres") %>% 
    mutate(timestamp = as_datetime(timestamp)) %>%
    mutate(year = year(timestamp)) %>% 
    left_join(b_y, by = "year") %>% 
    mutate(timestamp = as_datetime(timestamp)) %>%
    mutate(month = month(timestamp)) %>%
    left_join(b_m, by = "month") %>% 
    mutate(timestamp = as_datetime(timestamp)) %>%
    mutate(week = week(timestamp)) %>% 
    group_by(week) %>% 
    summarize(b_w = sum(rating -b_i-b_u-b_g-b_y-b_m-mu)/(n()+lambda))
  b_h <- edx_train_l %>% 
    left_join(b_i, by = "movieId") %>% 
    left_join(b_u, by = "userId") %>% 
    left_join(b_g, by = "genres") %>% 
    mutate(timestamp = as_datetime(timestamp)) %>%
    mutate(year = year(timestamp)) %>% 
    left_join(b_y, by = "year") %>% 
    mutate(timestamp = as_datetime(timestamp)) %>%
    mutate(month = month(timestamp)) %>%
    left_join(b_m, by = "month") %>% 
    mutate(timestamp = as_datetime(timestamp)) %>%
    mutate(week = week(timestamp)) %>% 
    left_join(b_w, by = "week") %>% 
    mutate(timestamp = as_datetime(timestamp)) %>%
    mutate(hour = hour(timestamp)) %>%
    group_by(hour) %>% 
    summarize(b_h = sum(rating -b_i-b_u-b_g-b_y-b_m-b_w-mu)/(n()+lambda))
  
  predicted_ratings <- 
    edx_test_l %>% 
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    left_join(b_g, by = "genres") %>% 
    mutate(timestamp = as_datetime(timestamp)) %>%
    mutate(year = year(timestamp)) %>% 
    left_join(b_y, by = "year") %>% 
    mutate(timestamp = as_datetime(timestamp)) %>%
    mutate(month = month(timestamp)) %>% 
    left_join(b_m, by = "month") %>% 
    mutate(timestamp = as_datetime(timestamp)) %>%
    mutate(week = week(timestamp)) %>% 
    left_join(b_w, by = "week") %>%
    mutate(timestamp = as_datetime(timestamp)) %>%
    mutate(hour= hour(timestamp)) %>% 
    left_join(b_h, by = "hour") %>% 
    filter(!is.na(b_i), !is.na(b_u),!is.na(b_g),!is.na(b_y),!is.na(b_m),
           !is.na(b_w),!is.na(b_h)) %>% 
    mutate(pred = mu + b_i + b_u +b_g+b_y+b_m+b_w+b_h) %>%
    pull(pred)
  return(RMSE(predicted_ratings, edx_test_l$rating))
}


lambdas <- seq(0,10,0.25)

lambdas_rmse <- sapply(lambdas,
                       regularization,
                       edx_train_l = edx_train_l,
                       edx_test_l = edx_test_l)

lambdas_tibble <- tibble(Lambda = lambdas, RMSE = lambdas_rmse)

lambdas_tibble



lambda <- lambdas[which.min(lambdas_rmse)]

```

```{r final reg cv plot, eval =T, echo =F,dev.args=list(bg="azure")}
library(gghighlight)
ggplot(data =lambdas_tibble,aes(x = Lambda, y = RMSE))+
  geom_point(col ="coral")+
  theme(panel.background = element_rect(fill = "cornsilk"), plot.background = element_rect(fill = "azure", colour ="azure"))+
  xlab("Lambdas")+
  ylab("Rmse")+
  gghighlight(RMSE == min(RMSE))
```

\
We will use a lambda value of `r lambda ` for our final regularized model. 

```{r final reg model, eval=T, echo = T, message =F, warning= F, error =F, results = "hide"}

mu <- mean(edx_train$rating)

b_i_reg <- edx_train %>% 
  group_by(movieId) %>%
  summarize(b_i = sum(rating - mu)/(n()+lambda))

b_u_reg <- edx_train %>% 
  left_join(b_i_reg, by="movieId") %>%
  group_by(userId) %>%
  summarize(b_u = sum(rating - b_i - mu)/(n()+lambda))

b_g_reg <- edx_train %>% 
  left_join(b_i_reg, by="movieId") %>% 
  left_join(b_u_reg, by="userId") %>% 
  group_by(genres) %>% 
  summarize(b_g = sum(rating -b_i-b_u-mu)/(n()+lambda))

b_y_reg <- edx_train %>% 
  left_join(b_i_reg, by="movieId") %>% 
  left_join(b_u_reg, by="userId") %>% 
  left_join(b_g_reg, by ="genres") %>% 
  mutate(timestamp = as_datetime(timestamp)) %>%
  mutate(year = year(timestamp)) %>% 
  group_by(year) %>% 
  summarize(b_y = sum(rating -b_i-b_u-b_g-mu)/(n()+lambda))

b_m_reg <-edx_train %>% 
  left_join(b_i_reg, by="movieId") %>% 
  left_join(b_u_reg, by="userId") %>% 
  left_join(b_g_reg, by ="genres") %>% 
  mutate(timestamp = as_datetime(timestamp)) %>%
  mutate(year = year(timestamp)) %>% 
  left_join(b_y_reg, by = "year") %>% 
  mutate(timestamp = as_datetime(timestamp)) %>%
  mutate(month = month(timestamp)) %>% 
  group_by(month) %>% 
  summarize(b_m = sum(rating -b_i-b_u-b_g-b_y-mu)/(n()+lambda))
  
b_w_reg <-edx_train %>% 
  left_join(b_i_reg, by="movieId") %>% 
  left_join(b_u_reg, by="userId") %>% 
  left_join(b_g_reg, by ="genres") %>% 
  mutate(timestamp = as_datetime(timestamp)) %>%
  mutate(year = year(timestamp)) %>% 
  left_join(b_y_reg, by = "year") %>%
  mutate(timestamp = as_datetime(timestamp)) %>%
  mutate(month = month(timestamp)) %>% 
  left_join(b_m_reg, by = "month") %>%
  mutate(timestamp = as_datetime(timestamp)) %>%
  mutate(week = week(timestamp)) %>% 
  group_by(week) %>% 
  summarize(b_w = sum(rating -b_i-b_u-b_g-b_y-b_m-mu)/(n()+lambda))

b_h_reg <-edx_train %>% 
  left_join(b_i_reg, by="movieId") %>% 
  left_join(b_u_reg, by="userId") %>% 
  left_join(b_g_reg, by ="genres") %>% 
  mutate(timestamp = as_datetime(timestamp)) %>%
  mutate(year = year(timestamp)) %>% 
  left_join(b_y_reg, by = "year") %>%
  mutate(timestamp = as_datetime(timestamp)) %>%
  mutate(month = month(timestamp)) %>% 
  left_join(b_m_reg, by = "month") %>%
  mutate(timestamp = as_datetime(timestamp)) %>%
  mutate(week = week(timestamp)) %>% 
  left_join(b_w_reg, by = "week") %>%
  mutate(timestamp = as_datetime(timestamp)) %>%
  mutate(hour = hour(timestamp)) %>% 
  group_by(hour) %>% 
  summarize(b_h = sum(rating -b_i-b_u-b_g-b_y-b_m-b_w-mu)/(n()+lambda))




preds <- edx_test %>% 
  left_join(b_i_reg, by = "movieId") %>% 
  left_join(b_u_reg, by = "userId") %>% 
  left_join(b_g_reg, by = "genres") %>%
  mutate(timestamp = as_datetime(timestamp)) %>%
  mutate(year = year(timestamp)) %>% 
  left_join(b_y_reg, by = "year") %>% 
  mutate(timestamp = as_datetime(timestamp)) %>%
  mutate(month = month(timestamp)) %>% 
  left_join(b_m_reg, by = "month") %>% 
  mutate(timestamp = as_datetime(timestamp)) %>%
  mutate(week = week(timestamp)) %>% 
  left_join(b_w_reg, by = "week") %>% 
  mutate(timestamp = as_datetime(timestamp)) %>%
  mutate(hour = hour(timestamp)) %>% 
  left_join(b_h_reg, by = "hour") %>% 
  mutate(prediction = mu + b_i +b_u+b_g+b_y+b_m+b_w+b_h) %>% 
  pull(prediction)

final_reg_model_rmse <- RMSE(preds, edx_test$rating)


rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Final Regularized Model",  
                                     RMSE = final_reg_model_rmse ))

```

\
With our final regularization model we were able to improve the RMSE to `r final_reg_model_rmse`, beating our target RMSE. 

Using a regularized linear regression approach to recommend ratings proved to result in an adequately performing model, while being sufficiently efficient and fairly easy to implement.

It is important to note here, that the winning team of the Netflix challenge, worked on a far bigger dataset. Even though we are using a sample from the same dataset used throughout the Netflix challenge, the difference in the sample size is enough to skew a direct comparison to their achieved RMSE. We can therefore not be satisfied with this minor improvement over our target error.


\
\

## 6.4 Matrix Factorization Model
\
Matrix Factorization is a popular technique used in recommendation systems. As described earlier, there are two types of recommendation systems. One being _content filtering_, the other being _collaborative filtering_. \
Throughout our analysis we have used _collaborative filtering_, which resulted in decent accuracy. We will build on this approach and further improve results by using the advanced method of Matrix Factorization.
These algorithms work by breaking down the user-item interaction matrix into two lower dimensionality rectangular matrices.

The winning team of the Netflix challenge also utilized this method by incorporating Matrix Factorization into their final submission, as part of a sophisticated blend of various different methods.

We will implement this approach in the form of a parallel stochastic gradient descent matrix factorization model with the use of the [recosystem package](https://github.com/yixuan/recosystem), an R wrapper of the LIBMF library. While being an incredibly powerful algorithm, it also combines computational efficiency with ease of implementation.

We will begin by converting our data into two sparse matrices as `DataSource` objects; a test and a train set. For that, we specify that our data is stored as an R object with the function `data_memory()`. The resulting matrices have three numbers per line, reducing sparsity and dimensionality.

We then initialize our model and fine-tune its parameters. After much consideration, we decided to use a combination of 20 and 30 latent factors, with the default learning rate and 10 iterations. Since this algorithm utilizes threads for parallel computing, we will set the thread count to 4. \
After the setup is complete, we will then train our model for 40 iterations before making predictions.


```{r mf model, eval = T, echo =T, message =F, warning= F, error =F, results = "hide"}
library(recosystem)
set.seed(123)

# Converting into recosystem input format:

train_reco <- with(edx_train,data_memory(user_index = userId,
                                         item_index = movieId,
                                         rating = rating))

test_reco <- with(edx_test, data_memory(user_index = userId,
                                        item_index = movieId,
                                        rating = rating))


# Creating model object:

mf_model <- Reco()

# Tuning model:

tuning <- mf_model$tune(train_reco, opts = list(dim = c(20,30),
                                              lrate = c(0.01,0.1),
                                              nthread = 4,
                                              niter = 10))
# Training model:

mf_model$train(train_reco, opts = c(tuning$min,
                                  nthread = 4,
                                  niter = 40))

# Prediction:
mf_preds <- mf_model$predict(test_reco, out_memory())


mf_model_rmse <- RMSE(mf_preds,edx_test$rating)


rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Matrix Factorization Model",  
                                     RMSE = mf_model_rmse ))

```

\
We managed to further improve the RMSE to `r mf_model_rmse` , making this model the first to significantly improve on our target RMSE. It achieved this while being fairly easy to implement and proving to be computationally efficient.



\
\
\

# 7 Final Validation
\
We are now ready to fit the best performing model, our Matrix Factorization model, to the complete dataset. The entirety of the training data (edx) will be used to train our previously evaluated and tuned algorithm, while calculating the final RMSE on the _final hold-out test set_. 

```{r final mf model, eval =T, echo =T, message =F, warning= F, error =F, results = "hide"}

### Matrix Factorization with Final holding test set

set.seed(123)

# Converting into recosystem input format:

train_reco <- with(edx_dat,data_memory(user_index = userId,
                                         item_index = movieId,
                                         rating = rating))

test_reco <- with(final_holdout_test, data_memory(user_index = userId,
                                        item_index = movieId))

# Note: 
# We omit the ratings for the test reco, since we would not have them in  
# practice


# Creating model object:

mf_model <- Reco()

# Tuning model:

tuning <- mf_model$tune(train_reco, opts = list(dim = c(20,30),
                                              lrate = c(0.01,0.1),
                                              nthread = 4,
                                              niter = 10))
# Training model:

mf_model$train(train_reco, opts = c(tuning$min,
                                  nthread = 4,
                                  niter = 40))

# Prediction:
mf_preds <- mf_model$predict(test_reco, out_memory())


final_mf_model_rmse <- RMSE(mf_preds,final_holdout_test$rating)



rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Final Matrix Factorization Model",  
                                     RMSE = final_mf_model_rmse ))

```

\
Thanks to the increase in training data, we were able to significantly improve our final RMSE to **`r final_mf_model_rmse`**, which is well below our initial target of **0.8649**.\
We achieved the goal we set at the beginning of this project, having found a highly accurate efficient model that is easy to implement.


\
We can now compare all of the models we have evaluated hitherto, to the very accurate final model:

```{r model comparisons, eval =T, echo =F}
library(knitr)
library(kableExtra)
# kable(rmse_results)

rmse_results %>% 
  kbl() %>% 
  kable_material_dark(full_width = F) %>% 
  column_spec(2, color = "white", 
              background = spec_color(rmse_results$RMSE[1:17], end = 0.7,
                                      direction = -1,option="E"))

```

\


```{r model comparison plot 1, eval =T, echo = F,dev.args=list(bg="azure"), fig.cap ="Higher is worse"}
rmse_results %>% 
  ggplot(aes(method,RMSE, fill = RMSE))+
  geom_bar(stat = "identity")+
  theme(axis.text.x = element_text(angle=70,hjust = 1,size = 5))+
  coord_cartesian(ylim = c(0.5,1.25))+
  theme(panel.background = element_rect(fill = "cornsilk"), plot.background = element_rect(fill = "azure", colour ="azure"))+
  xlab("Method")+
  ggtitle("Model Evaluation")+
  scale_fill_gradient2(low="coral",high = "azure4", mid = "azure3",
                       midpoint = 0.9)


```

\


```{r model comparison plot 2, eval =T, echo = F,dev.args=list(bg="azure"), fig.cap ="Performance based"}
rmse_results %>% 
  mutate(RMSE = 2- RMSE) %>% 
  arrange(desc(RMSE)) %>% 
  ggplot(aes(RMSE,reorder(method,RMSE), fill = RMSE))+
  geom_bar(stat = "identity")+
  theme(axis.title.y = element_blank())+
  theme(panel.background = element_rect(fill = "cornsilk"), plot.background = element_rect(fill = "azure", colour ="azure"))+
  coord_cartesian(xlim = c(0.4,1.25))+
  scale_x_continuous(breaks= c(0.4,0.6,0.8,1.0,1.2), labels= c(1.6,1.4,1.2,1,0.8))+
  scale_fill_gradient2(low="azure4",high = "coral", mid = "azure3",
                       midpoint = 1.1,breaks= c(0.6,0.8,1.0,1.2), 
                       labels= c(1.4,1.2,1,"below target"))
          
```

\

As we can observe, we began our evaluation process with baseline models, gradually improving our prediction accuracy with each subsequent model, and finally arriving at the Matrix Factorization model we used for our final validation.



\
\
\

# 8 Recommenderlab Models
\
Similarly to the previously used `recosystem` package, the `recommenderlab` package includes various recommender algorithms that can be implemented to predict ratings. The supported algorithms in the `recommenderlab` package can, aside from predicting ratings, also give top movie recommendations for users. A feature we will not demonstrate here.

To finish off our project we will illustrate and evaluate several models found in the `recommenderlab` package. Our focus will be on two algorithms - _User-based collaborative filtering_ (UBCF) and _Item-based collaborative filtering_ (IBCF). We will then merge two algorithms into a _hybrid system_ to enhance the algorithm’s predictive capabilities.

The reason we include the `recommenderlab` models after we already conducted the final validation, is that it was decided beforehand, that the algorithms in this package will not be viable for our use case. These models require the dataset to have less sparsity and do not perform well with larger datasets, as is the case with the dataset we are currently working with. Therefore, implementing these models is not feasible for our analysis, especially when our goal is to achieve somewhat acceptable prediction accuracy within a reasonable timeframe, as we will exemplify later.

To that extent, we will demonstrate these algorithms on a small portion of our actual dataset that only includes the most relevant users and movies that have been rated and reviewed the most. We will show how the prediction accuracy and computation time progressively worsen as we use more data to train, and test the model.\
Despite that, there is value in illustrating how these models can be used, and exploring how they compare to previously used models.

\
Before beginning, we need to load the necessary libraries that were not used so far during this project, if we have not done so already:

```{r loading recommenderlab libs, eval =T, echo =T, message =F, warning= F, error =F, results = "hide"}
library(recommenderlab)
library(reshape2)
```



\
\

## 8.1 Converting Data into realRatingMatrix
\
The first step will be to create a **sparse matrix** of our data and converting said matrix into a `realRatingMatrix`, a sparse matrix that is needed as input for the algorithms we will be working with.

We will then, select only the most relevant users (users that rated the most movies) and the most relevant movies (movies that were rated the most), as explained earlier. We will select movies and users in the 90th quantile.

```{r converting data into sparse matrix, eval =T, echo =T, message =F, warning= F, error =F, results = "hide"}

# Creating realRatingMatrix and creating subset of data with only 
# relevant data

edx.copy <- edx

edx.copy$userId <- as.factor(edx.copy$userId)
edx.copy$movieId <- as.factor(edx.copy$movieId)



sparse_dat <- sparseMatrix(i = edx.copy$userId,
                           j = edx.copy$movieId,
                           x = edx.copy$rating,
                           dims = c(length(unique(edx.copy$userId)),
                                    length(unique(edx.copy$movieId))),
                           dimnames= list(paste("u", 
                                                1:length(unique(edx.copy$userId)),
                                                sep = ""),
                                          paste("m",
                                                1:length(unique(edx.copy$movieId)), 
                                                sep = "")))

rm(edx.copy)



r_mat <- as(sparse_dat, "realRatingMatrix")


# Selecting the most relevant users and movies:

# Determining the minimum number of movies per user:
min_movies <- quantile(rowCounts(r_mat),0.9)
min_movies

min_users <- quantile(colCounts(r_mat),0.9)

# Only having upper bound of users and movies

rel_r_mat <- r_mat[rowCounts(r_mat) > min_movies,
                   colCounts(r_mat) > min_users]

```



\
\

## 8.2 Initial Model Comparison
\
The `recommenderlab` package includes a multitude of available models that can be seen [here](https://www.rdocumentation.org/packages/recommenderlab/versions/1.0.6). Each algorithm has its own strengths and weaknesses, so it is important to choose the one that best suits the task at hand. For the purpose of this analysis, we chose to closer examine these four models in particular:


- **User-based-collaborative filtering** utilizing **Cosine similarity** as similarity measure

- **User-based-collaborative filtering** utilizing **Pearson correlation** as similarity measure

- **Item-based-collaborative filtering** utilizing **Cosine similarity** as similarity measure 

- **Item-based-collaborative filtering** utilizing **Pearson correlation** as similarity measure

\
\
The `recommenderlab` package enables us to evaluate multiple algorithms simultaneously, streamlining the process and allowing us to compare their performances. This way, we can determine which algorithm is most suited for our analysis.

```{r stacked rl models, eval =T, echo =T, class.source ="fold-hide", message =F, warning= F, error =F, results = "hide"}

set.seed(1)


models_train_scheme <- rel_r_mat%>%
  evaluationScheme(method = 'cross-validation',
                   given = -5, 
                   k = 10)


models_to_try <- list(
  `IBCF Cosinus` = list(name = "IBCF",
                        param = list(method = "cosine")),
  `IBCF Pearson` = list(name = "IBCF",
                        param = list(method = "pearson")),
  `UBCF Cosinus` = list(name = "UBCF",
                        param = list(method = "cosine")),
  `UBCF Pearson` = list(name = "UBCF",
                        param = list(method = "pearson"))
)



results <- evaluate(models_train_scheme, method = models_to_try,
                    type = "ratings")

```

```{r stacked rl models plot, eval =T, echo =F}
plot(results)
```

\
We have observed that the IBCF model, specifically with Pearson correlation, performs the best. The UBCF model, with Cosine similarity, ranks second. We will keep this in mind when we evaluate each model individually.



\
\

## 8.3 Model Evaluation
\
Our next step is to evaluate the four chosen models separately. To ensure reproducibility, we set the seed and create a **model training scheme** using an object of class `evaluationScheme`. This allows us to split the data into train and test sets, and randomly select given ratings for each user to present to the recommender algorithm. For all models, we will select all ratings except five and train the model on 75% of the data. 

This process will remain consistent for all models.


\
\

### 8.3.1 UBCF - Cosine
\
We proceed to build our model by utilizing the training scheme we created earlier, supplying the model with the required parameters.  We will normalize the data since we have not done so beforehand, using the “center” method for scaling. Additionally, we set _“nn”_ (neighborhood of users) to 150 based on cross-validation.\
Afterwards, we predict ratings (type = “ratings”) , using the “known” ratings for prediction. To obtain our prediction accuracy we compare the results to our unknown data.

Finally, we save the results of the prediction accuracy calculation to a dataframe.

```{r ubcf cosine, eval =T, echo = T, message =F, warning= F, error =F, results = "hide"}

set.seed(1)
model_train_scheme <- rel_r_mat %>%
  evaluationScheme(method = "split", # single train/test split
                   train = 0.75, # proportion of rows to train.
                   given = -5) 

# Model



ubcf_model <- getData(model_train_scheme, "train") %>% # Only fit on the 75% training data
  Recommender(method = "UBCF", param = list(method = "Cosine",
                                            nn = 150,
                                            normalize = "center"))



# Prediction

ubcf_pred <- predict(ubcf_model, getData(model_train_scheme, "known"), type = "ratings")
ubcf_pred




test_error_ubcf_c <- calcPredictionAccuracy(ubcf_pred, getData(model_train_scheme, "unknown"))[1]

recom_results <- data_frame(method = "UBCF Cosine", RMSE = test_error_ubcf_c)

```



\
\

### 8.3.2 UBCF - Pearson
\
After, the same process is repeated, this time using Pearson correlation as similarity measure. Here we set _“nn”_  to 200, based on cross-validation. 

Just as we did with our first model, we save the resulting RMSE, adding it to the previously created dataframe.

```{r ubcf pearson model, eval =T, echo =T, message =F, warning= F, error =F, results = "hide"}

set.seed(1)
model_train_scheme <- rel_r_mat %>%
  evaluationScheme(method = "split",
                   train = 0.75, 
                   given = -5) 
# Model


ubcf_model <- getData(model_train_scheme, "train") %>%
  Recommender(method = "UBCF", param = list(method = "Pearson",
                                            nn = 200,
                                            normalize = "center"))



# Prediction

ubcf_pred <- predict(ubcf_model, getData(model_train_scheme, "known"), type = "ratings")
ubcf_pred




test_error_ubcf_p <- calcPredictionAccuracy(ubcf_pred, getData(model_train_scheme, "unknown"))[1]

recom_results <- bind_rows(recom_results,
                           data_frame(method = "UBCF Pearson",
                                      RMSE = test_error_ubcf_p))

```



\
\

### 8.3.3 IBCF - Cosine
\
Instead of basing our predictions on the most similar users to the one that is being predicted, we base it on the closest movies (items) to the item in question. The model code looks identical for the most part. We change the method to _IBCF_ and _Cosine_, respectively, and change the _“k”_ parameter to 275 based on cross-validation. The _k_ parameter describes the number of the most similar items that are being identified.

```{r ibcf cosine model, eval =T, echo =T, message =F, warning= F, error =F, results = "hide"}


set.seed(1)

model_train_scheme <- rel_r_mat %>%
  evaluationScheme(method = 'split', # single train/test split
                   train = 0.75, 
                   given = -5,
                   k = 1)


# Model

model_params <- list(method = "Cosine",
                     k = 275, # Based on cross-validation
                     normalize = "center")

ibcf_model <- getData(model_train_scheme, "train") %>% 
  Recommender(method = "IBCF", param = model_params)



# Prediction

ibcf_pred <- predict(ibcf_model, getData(model_train_scheme, "known"), type = "ratings")
ibcf_pred




test_error_ibcf_c <- calcPredictionAccuracy(ibcf_pred, getData(model_train_scheme, "unknown"))[1]

recom_results <- bind_rows(recom_results,
                           data_frame(method = "IBCF Cosine", 
                                      RMSE = test_error_ibcf_c))

```

\
We save the results to our results-dataframe for later comparison.



\
\

### 8.3.4 IBCF - Pearson
\
The last algorithm to be evaluated is the item-based-collaborative filtering method with Pearson correlation. It will be interesting to see how this algorithm holds up, since it performed the best in our initial exploration. 

The code remains the same, baring the change to the _“k”_ parameter to 50 and the obligatory changes to the methods used.

Once again, the results are saved, as previously stated.

```{r ibcf pearson model, eval =T, echo =T, message =F, warning= F, error =F, results = "hide"}

set.seed(1)

model_train_scheme <- rel_r_mat %>%
  evaluationScheme(method = 'split', # single train/test split
                   train = 0.75, 
                   given = -5,
                   k = 1)


# Model

model_params <- list(method = "Pearson",
                     k = 50, # Based on cross-validation
                     normalize = "center")

ibcf_model <- getData(model_train_scheme, "train") %>% 
  Recommender(method = "IBCF", param = model_params)



# Prediction

ibcf_pred <- predict(ibcf_model, getData(model_train_scheme, "known"), type = "ratings")
ibcf_pred




test_error_ibcf_p <- calcPredictionAccuracy(ibcf_pred, getData(model_train_scheme, "unknown"))[1]

recom_results <- bind_rows(recom_results,
                           data_frame(method = "IBCF Pearson", 
                                      RMSE = test_error_ibcf_p))

```



\
\

### 8.3.5 Hybrid Recommender / Ensemble
\
Upon reviewing the individual results of the models, we have arrived at the same conclusion as our initial evaluation. Both the initial analysis of the selected models, and the individual model evaluation indicate that the **IBCF - Pearson** and **UBCF - Cosine** models deliver the best performance in our scenario.

We will use these two models to create a hybrid model of the two, improving the prediction accuracy further, by combining both of their strengths. We have already decided the parameters for both models, hence we will keep it simple and use those values. In the calculations, we will weigh both models equally.

Naturally, the results are saved onto the dataframe for comparison.

```{r hybrid rl model, eval =T, echo =T, message =F, warning= F, error =F, results = "hide"}

set.seed(1)

model_train_scheme <- rel_r_mat %>%
  evaluationScheme(method = 'split', # single train/test split
                   train = 0.75, 
                   given = -5,
                   k = 1)


# Model

recommenders <- list(
  UBCF = list(name = "UBCF", param = list(method = "Cosine",
                                       nn = 150,
                                       normalize = "center")),
  IBCF = list(name = "IBCF", param = list(method = "Pearson",
                                          k = 50, 
                                          normalize = "center"))
)



hyb_recom <- getData(model_train_scheme, "train") %>% 
  Recommender(method = "HYBRID", parameter = list(recommenders = recommenders, 
                                                  weights = c(0.5,0.5)))



# Prediction

hyb_pred <- predict(hyb_recom, getData(model_train_scheme, "known"), type = "ratings")
hyb_pred




test_error_hyb <- calcPredictionAccuracy(hyb_pred, getData(model_train_scheme, "unknown"))[1]

recom_results <- bind_rows(recom_results,
                           data_frame(method = "HYBRID",
                                      RMSE = test_error_hyb))

```



\
\

## 8.4 Final Comparison
\
```{r rl model comparison, eval =T, echo =F}

library(kableExtra)

# kable(recom_results)

recom_results %>% 
  kbl() %>% 
  kable_material_dark(full_width = F) %>% 
  column_spec(2, color = "white", 
              background = spec_color(recom_results$RMSE[1:5], end = 0.7,
                                      direction = -1))
```

\
Unsurprisingly, we observe the Hybrid model performing the best. As mentioned before, the IBCF model with Pearson correlation achieves the highest accuracy when comparing the individual models to one another. UBCF with Cosine similarity is not far off, coming in second. The Hybrid recommender improves significantly on these results, pairing the two best performing individual models into a joint model, and surpassing the prediction accuracy of our very powerful Matrix Factorization model, making it the best performing model discussed throughout this analysis.

\
```{r rl model comparison plot 1, eval =T, echo =F,dev.args=list(bg="azure"), fig.cap ="Higher is worse"}

# Higher is worse
recom_results %>% 
  ggplot(aes(method,RMSE, fill = RMSE))+
  geom_bar(stat = "identity")+
  theme(axis.text.x = element_text(angle=70,hjust = 1,size = 5))+
  coord_cartesian(ylim = c(0.5,1.25))+
  theme(panel.background = element_rect(fill = "cornsilk"), plot.background = element_rect(fill = "azure", colour ="azure"))+
  xlab("Method")+
  ggtitle("Recommenderlab Models")+
  scale_fill_gradient2(low="coral",high = "azure4", mid = "azure3",
                       midpoint = 0.8)
```

\


```{r rl model comparison plot 2, eval = T, echo =F,dev.args=list(bg="azure"),fig.cap ="Performance based"}

# Performance based
recom_results %>% 
  mutate(RMSE = 1.5- RMSE) %>% 
  arrange(desc(RMSE)) %>% 
  ggplot(aes(RMSE,reorder(method,RMSE), fill = RMSE))+
  geom_bar(stat = "identity")+
  theme(axis.title.y = element_blank())+
  theme(panel.background = element_rect(fill = "cornsilk"), plot.background = element_rect(fill = "azure", colour ="azure"))+
  coord_cartesian(xlim = c(0.5,0.8))+
  scale_x_continuous(breaks= c(0.5,0.6,0.7,0.8), labels= c(1,0.9,0.8,0.7))+
  scale_fill_gradient2(low="azure4",high = "coral", mid = "azure3",
                       midpoint = 0.7 ,breaks= c(0.71,0.7,0.69), 
                       labels= c(0.79,0.8,0.81))

```

\
All models perform admirably when we compare their results to the Baseline models, the Regularized Linear models, the `recosystem` Matrix Factorization model and our target RMSE.

However, they only do so when we ignore the very important caveat of the reduced amount of data used for training and testing. We only used a small subset of the most relevant data, and completely ignored the portion of the data for which predictions are most difficult to make, since there is less information available for those data points.

For that reason, we cannot compare the results of these `recommenderlab` algorithms to the aforementioned algorithms formerly used. These algorithms might have value when working with a dataset with less sparsity, but for most practical applications they are not viable.

In the following section we will illustrate how these algorithms behave when more, less relevant, data is supplied. 


\
\

## 8.5 Examining Different Dataset Sizes
\
To illustrate how the `recommenderlab` algorithms cannot be used with sparse data as the one we are working with, we will show how the results of the UBCF – Cosine model get **progressively worse with the increase in data** used for testing and training.

We will be using different quantile cutoffs of users with the most movie ratings and movies with the most user ratings, to demonstrate the effect the change in subset size has on the prediction accuracy of the model. Using more of the data results in increasingly higher RMSE and progressively longer model run times.



\
\

### 8.5.1 Creating Matrices with Different Quantile Cutoffs
\
We first create a sparse matrix of our data as we did before, then we convert it into a `realRatingMatrix`.

Afterwards, we will create separate matrices for each quantile cutoff before running the models.  We will start with the 90th quantile, which was used in the previous analysis, and then move down in intervals of 5%, until the 70th quantile is reached. This will result in five individual models.

```{r creating matrices with different cutoffs, eval =T, echo =T, class.source ="fold-hide", message =F, warning= F, error =F, results = "hide"}


edx.copy <- edx

edx.copy$userId <- as.factor(edx.copy$userId)
edx.copy$movieId <- as.factor(edx.copy$movieId)



sparse_dat <- sparseMatrix(i = edx.copy$userId,
                           j = edx.copy$movieId,
                           x = edx.copy$rating,
                           dims = c(length(unique(edx.copy$userId)),
                                    length(unique(edx.copy$movieId))),
                           dimnames= list(paste("u", 
                                                1:length(unique(edx.copy$userId)),
                                                sep = ""),
                                          paste("m",
                                                1:length(unique(edx.copy$movieId)), 
                                                sep = "")))

rm(edx.copy)


r_mat <- as(sparse_dat, "realRatingMatrix")


############### 90% quantile cutoff

# Determining the minimum number of movies per user:
min_movies <- quantile(rowCounts(r_mat),0.9)
min_movies

min_users <- quantile(colCounts(r_mat),0.9)

# Only having upper bound of users and movies

rel_r_mat_90 <- r_mat[rowCounts(r_mat) > min_movies,
                   colCounts(r_mat) > min_users]


############### 85% quantile cutoff

# Determining the minimum number of movies per user:
min_movies <- quantile(rowCounts(r_mat),0.85)
min_movies

min_users <- quantile(colCounts(r_mat),0.85)

# Only having upper bound of users and movies

rel_r_mat_85 <- r_mat[rowCounts(r_mat) > min_movies,
                      colCounts(r_mat) > min_users]


############### 80% quantile cutoff

# Determining the minimum number of movies per user:
min_movies <- quantile(rowCounts(r_mat),0.8)
min_movies

min_users <- quantile(colCounts(r_mat),0.8)

# Only having upper bound of users and movies

rel_r_mat_80 <- r_mat[rowCounts(r_mat) > min_movies,
                      colCounts(r_mat) > min_users]



############### 75% quantile cutoff

# Determining the minimum number of movies per user:
min_movies <- quantile(rowCounts(r_mat),0.75)
min_movies

min_users <- quantile(colCounts(r_mat),0.75)

# Only having upper bound of users and movies

rel_r_mat_75 <- r_mat[rowCounts(r_mat) > min_movies,
                      colCounts(r_mat) > min_users]


############### 70% quantile cutoff

# Determining the minimum number of movies per user:
min_movies <- quantile(rowCounts(r_mat),0.7)
min_movies

min_users <- quantile(colCounts(r_mat),0.7)

# Only having upper bound of users and movies

rel_r_mat_70 <- r_mat[rowCounts(r_mat) > min_movies,
                      colCounts(r_mat) > min_users]

```



\
\

### 8.5.2 Models with Different Cutoffs
\
We can now execute each model and store their outcomes into a dataframe. The code will remain the same as before. Additionally, we can run a model that makes predictions for the entire dataset without any cutoffs. However, we do not recommend running this model as it takes approximately 8 hours to complete.

```{r ubcf models with different cutoffs, eval =T, echo =T, class.source = "fold-hide", message =F, warning= F, error =F, results = "hide"}

set.seed(1)
model_train_scheme <- rel_r_mat_90 %>%
  evaluationScheme(method = "split",
                   train = 0.75,
                   given = -5) 

ubcf_model <- getData(model_train_scheme, "train") %>% 
  Recommender(method = "UBCF", param = list(method = "Cosine",
                                            nn = 50,
                                            normalize = "center"))

pred_90 <- predict(ubcf_model, getData(model_train_scheme, "known"), type = "ratings")


rmse_90 <- calcPredictionAccuracy(pred_90, getData(model_train_scheme, "unknown"))[1]

cutoff_comp <- data_frame(cutoff = "90%", RMSE = rmse_90)


model_train_scheme <- rel_r_mat_85 %>%
  evaluationScheme(method = "split",
                   train = 0.75,
                   given = -5) 

ubcf_model <- getData(model_train_scheme, "train") %>% 
  Recommender(method = "UBCF", param = list(method = "Cosine",
                                            nn = 50,
                                            normalize = "center"))

pred_85 <- predict(ubcf_model, getData(model_train_scheme, "known"), type = "ratings")


rmse_85 <- calcPredictionAccuracy(pred_85, getData(model_train_scheme, "unknown"))[1]

cutoff_comp <- bind_rows(cutoff_comp,
                         data_frame(cutoff = "85%", 
                                    RMSE = rmse_85))


model_train_scheme <- rel_r_mat_80 %>%
  evaluationScheme(method = "split",
                   train = 0.75,
                   given = -5) 

ubcf_model <- getData(model_train_scheme, "train") %>% 
  Recommender(method = "UBCF", param = list(method = "Cosine",
                                            nn = 50,
                                            normalize = "center"))

pred_80 <- predict(ubcf_model, getData(model_train_scheme, "known"), type = "ratings")


rmse_80 <- calcPredictionAccuracy(pred_80, getData(model_train_scheme, "unknown"))[1]

cutoff_comp <- bind_rows(cutoff_comp,
                         data_frame(cutoff = "80%", 
                                    RMSE = rmse_80))

model_train_scheme <- rel_r_mat_75 %>%
  evaluationScheme(method = "split",
                   train = 0.75,
                   given = -5) 

ubcf_model <- getData(model_train_scheme, "train") %>% 
  Recommender(method = "UBCF", param = list(method = "Cosine",
                                            nn = 50,
                                            normalize = "center"))

pred_75 <- predict(ubcf_model, getData(model_train_scheme, "known"), type = "ratings")


rmse_75 <- calcPredictionAccuracy(pred_75, getData(model_train_scheme, "unknown"))[1]

cutoff_comp <- bind_rows(cutoff_comp,
                         data_frame(cutoff = "75%", 
                                    RMSE = rmse_75))


model_train_scheme <- rel_r_mat_70 %>%
  evaluationScheme(method = "split",
                   train = 0.75,
                   given = -5) 

ubcf_model <- getData(model_train_scheme, "train") %>% 
  Recommender(method = "UBCF", param = list(method = "Cosine",
                                            nn = 50,
                                            normalize = "center"))

pred_70 <- predict(ubcf_model, getData(model_train_scheme, "known"), type = "ratings")


rmse_70 <- calcPredictionAccuracy(pred_70, getData(model_train_scheme, "unknown"))[1]

cutoff_comp <- bind_rows(cutoff_comp,
                         data_frame(cutoff = "70%", 
                                    RMSE = rmse_70))

```

```{r no cutoffs ubcf model, eval =T, echo =T, class.source = "fold-hide"}

### NO CUTOFF
# Do not run this code. It takes about 8 hours and achieves a horrendous RMSE
# RMSE : 


# 
# set.seed(1)
# model_train_scheme <- r_mat %>%
#   evaluationScheme(method = "split",
#                    train = 0.75,
#                    given = -5) 
# 
# ubcf_model <- getData(model_train_scheme, "train") %>% 
#   Recommender(method = "UBCF", param = list(method = "Cosine",
#                                             nn = 50,
#                                             normalize = "center"))
# 
# pred <- predict(ubcf_model, getData(model_train_scheme, "known"), type = #"ratings")
# 
# 
# rmse <- calcPredictionAccuracy(pred, getData(model_train_scheme,
# "unknown"))[1]



cutoff_comp <-bind_rows(cutoff_comp,
                        data_frame(cutoff = "Complete Dataset", 
                                   RMSE = 1.225969))

```




\
\

### 8.5.3 Cutoff Results and Comparison
\
We observe a steep, progressive downfall in prediction accuracy, culminating in the results we achieve when omitting any cutoffs. We achieve an RMSE that is on the level of our baseline models.

At the 80th percentile cutoff, our prediction accuracy is already worse than that of our final regularized linear model.

Besides that, the model runtimes share the trend of the prediction accuracy, in that they get incrementally worse, becoming downright atrocious when trying to run this model on the whole dataset. This demonstrates how these algorithms are not suitable for the project's objectives, and should only be utilized when working with minimal sparsity.

```{r cutoff table, eval =T, echo =F}
library(knitr)
library(kableExtra)

# kable(cutoff_comp)

cutoff_comp %>% 
  kbl() %>% 
  kable_material_dark(full_width = F) %>% 
  column_spec(2, color = "white", 
              background = spec_color(cutoff_comp$RMSE[1:6], end = 0.7,
                                      direction = -1,option="E"))

```

\

```{r cutoffs comparison plot, eval =T, echo=F,dev.args=list(bg="azure")}
cutoff_comp %>% 
  mutate(RMSE = 1.5- RMSE) %>% 
  arrange(desc(RMSE)) %>% 
  ggplot(aes(RMSE,reorder(cutoff,RMSE), fill = RMSE))+
  geom_bar(stat = "identity")+
  theme(panel.background = element_rect(fill = "cornsilk"), plot.background = element_rect(fill = "azure", colour ="azure"))+
  ylab("Cutoff")+
  coord_cartesian(xlim = c(0.2,0.8))+
  scale_x_continuous(breaks= c(0.2,0.5,0.6,0.7,0.8), labels= c(1.3,1,0.9,0.8,0.7))+
  scale_fill_gradient2(low="azure4",high = "coral", mid = "azure3",
                       midpoint = 0.65 ,breaks= c(0.3,0.6,0.7), 
                       labels= c(1.2,0.9,0.80))

```



\
\
\

# 9 Conclusion
\
Throughout this analysis, we were able to explain the process of developing a recommendation system, and explored different models to predict movie ratings based on the 10M MovieLens dataset.

The model evaluation performance through the RMSE showed that the Linear Regression models with regularized effects are viable Recommender systems to predict ratings. However, the implementation of `recommenderlab` models proved to be too computationally intensive without providing any improvement in performance to be considered a viable option as a Recommender system. As we have demonstrated, these algorithms are only effective when working with smaller datasets or when subsetting the data to exclusively include the users, or items, with the most datapoints.

Out of the models built, only the matrix factorization model implemented with  `recosystem` achieved a significantly lower Root Mean Squared Error (RMSE) than the target we set. This was expected because linear models tend to have underfitting issues in non-linear data. Despite being the best performing model, it was also highly computationally efficient and easy to implement.

Research suggests that there are opportunities to further improve model performance. For instance, using the [`h2o`](https://github.com/h2oai/h2o-3/blob/master/README.md) package for model implementation, or, a complex mix of algorithms. The latter was the case for the winning team of the Netflix challenge. Implementing such an approach, while utilizing more powerful underlying systems, could potentially yield greater results. Additionally, theory suggests that an [ensemble blended with Fusion-Support Vector Regression](https://fardpaper.ir/mohavaha/uploads/2017/11/Combining-User-Based-and-Item-Based-Collaborative-Filtering-Using-Machine-Learning.pdf) could be a potent technique. However, these approaches are beyond the scope of this project, despite their potential.

\
\
\





